{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbccc498",
   "metadata": {},
   "source": [
    "# HMM POS Tagger\n",
    "This notebook trains a Hidden Markov Model (HMM) POS tagger on the provided CONLLU data, evaluates on the dev and test files, and prints performance metrics.\n",
    "Files used: `PosData/fa_perdt-ud-train.conllu`, `PosData/fa_perdt-ud-dev.conllu`, `PosData/fa_perdt-ud-test.conllu`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfaf628",
   "metadata": {},
   "source": [
    "## Dependencies (run this cell once to install)\n",
    "pip install hmmlearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67e8d0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Install required packages if necessary. Uncomment to run within the notebook.\n",
    "# Note: running installs inside notebooks may require a restart of the kernel in some setups.\n",
    "# Uncomment the following line if pyconll or scikit-learn are missing:\n",
    "# !{sys.executable} -m pip install pyconll scikit-learn numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4786df",
   "metadata": {},
   "source": [
    "## Load data and implement HMM (training, Viterbi, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edce8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train sentences: 20913, Dev sentences: 1162, Test sentences: 1212\n",
      "Training HMM model...\n",
      "Train sentences: 20913, Dev sentences: 1162, Test sentences: 1212\n",
      "Training HMM model...\n",
      "HMM Training complete.\n",
      "HMM Training complete.\n",
      "\n",
      "========== Evaluation on Development (Dev) Set ==========\n",
      "Overall Tagging Accuracy: 0.9084587441619097\n",
      "\n",
      "Detailed Metrics (Precision, Recall, F1-Score) per UPOS Tag:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ     0.8472    0.7879    0.8165      1485\n",
      "         ADP     0.9053    0.9873    0.9445      2749\n",
      "         ADV     0.9064    0.8313    0.8672       326\n",
      "         AUX     0.9207    0.9744    0.9468       703\n",
      "       CCONJ     0.9565    0.9976    0.9766       838\n",
      "         DET     0.7793    0.9333    0.8494       420\n",
      "        INTJ     0.9500    0.5938    0.7308        32\n",
      "        NOUN     0.9344    0.8871    0.9101      6530\n",
      "         NUM     0.9251    0.8607    0.8917       244\n",
      "        PART     1.0000    0.6875    0.8148        16\n",
      "        PRON     0.8694    0.9450    0.9056       655\n",
      "       PROPN     0.7875    0.6684    0.7231       959\n",
      "       PUNCT     0.9978    0.9950    0.9964      1800\n",
      "       SCONJ     0.9162    0.9757    0.9450       493\n",
      "        VERB     0.8643    0.9272    0.8947      2020\n",
      "\n",
      "    accuracy                         0.9085     19270\n",
      "   macro avg     0.9040    0.8701    0.8809     19270\n",
      "weighted avg     0.9087    0.9085    0.9073     19270\n",
      "\n",
      "\n",
      "Confusion Matrix (Labels Order):\n",
      "Labels: ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'VERB', 'X']\n",
      "[[1170   50    9   10    9   10    0  124    0    0    1   32    1    5\n",
      "    64    0]\n",
      " [   3 2714    0    0    0    0    0   25    0    0    0    0    0    5\n",
      "     2    0]\n",
      " [  13   12  271    0    0    3    0   10    0    0   11    4    0    1\n",
      "     1    0]\n",
      " [   3    1    0  685    1    0    0    2    0    0    1    0    0    0\n",
      "    10    0]\n",
      " [   0    0    2    0  836    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    1    0    0  392    0   11    0    0   16    0    0    0\n",
      "     0    0]\n",
      " [   1    2    2    0    0    0   19    4    0    0    0    1    0    2\n",
      "     1    0]\n",
      " [ 128  125   12   30   22   49    1 5793   14    0   43  126    1   20\n",
      "   166    0]\n",
      " [   1   13    0    0    0    1    0   14  210    0    1    1    0    0\n",
      "     3    0]\n",
      " [   1    0    0    0    0    0    0    0    0   11    0    0    0    4\n",
      "     0    0]\n",
      " [   0    0    1    0    0   26    0    8    0    0  619    0    0    0\n",
      "     1    0]\n",
      " [  36   29    0    5    3   18    0  167    2    0    8  641    0    6\n",
      "    44    0]\n",
      " [   1    1    0    1    0    0    0    4    0    0    0    0 1791    0\n",
      "     2    0]\n",
      " [   0   11    1    0    0    0    0    0    0    0    0    0    0  481\n",
      "     0    0]\n",
      " [  24   40    0   13    3    4    0   38    1    0   12    9    2    1\n",
      "  1873    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n",
      "==================================================\n",
      "\n",
      "========== Evaluation on Development (Dev) Set ==========\n",
      "Overall Tagging Accuracy: 0.9084587441619097\n",
      "\n",
      "Detailed Metrics (Precision, Recall, F1-Score) per UPOS Tag:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ     0.8472    0.7879    0.8165      1485\n",
      "         ADP     0.9053    0.9873    0.9445      2749\n",
      "         ADV     0.9064    0.8313    0.8672       326\n",
      "         AUX     0.9207    0.9744    0.9468       703\n",
      "       CCONJ     0.9565    0.9976    0.9766       838\n",
      "         DET     0.7793    0.9333    0.8494       420\n",
      "        INTJ     0.9500    0.5938    0.7308        32\n",
      "        NOUN     0.9344    0.8871    0.9101      6530\n",
      "         NUM     0.9251    0.8607    0.8917       244\n",
      "        PART     1.0000    0.6875    0.8148        16\n",
      "        PRON     0.8694    0.9450    0.9056       655\n",
      "       PROPN     0.7875    0.6684    0.7231       959\n",
      "       PUNCT     0.9978    0.9950    0.9964      1800\n",
      "       SCONJ     0.9162    0.9757    0.9450       493\n",
      "        VERB     0.8643    0.9272    0.8947      2020\n",
      "\n",
      "    accuracy                         0.9085     19270\n",
      "   macro avg     0.9040    0.8701    0.8809     19270\n",
      "weighted avg     0.9087    0.9085    0.9073     19270\n",
      "\n",
      "\n",
      "Confusion Matrix (Labels Order):\n",
      "Labels: ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'VERB', 'X']\n",
      "[[1170   50    9   10    9   10    0  124    0    0    1   32    1    5\n",
      "    64    0]\n",
      " [   3 2714    0    0    0    0    0   25    0    0    0    0    0    5\n",
      "     2    0]\n",
      " [  13   12  271    0    0    3    0   10    0    0   11    4    0    1\n",
      "     1    0]\n",
      " [   3    1    0  685    1    0    0    2    0    0    1    0    0    0\n",
      "    10    0]\n",
      " [   0    0    2    0  836    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    1    0    0  392    0   11    0    0   16    0    0    0\n",
      "     0    0]\n",
      " [   1    2    2    0    0    0   19    4    0    0    0    1    0    2\n",
      "     1    0]\n",
      " [ 128  125   12   30   22   49    1 5793   14    0   43  126    1   20\n",
      "   166    0]\n",
      " [   1   13    0    0    0    1    0   14  210    0    1    1    0    0\n",
      "     3    0]\n",
      " [   1    0    0    0    0    0    0    0    0   11    0    0    0    4\n",
      "     0    0]\n",
      " [   0    0    1    0    0   26    0    8    0    0  619    0    0    0\n",
      "     1    0]\n",
      " [  36   29    0    5    3   18    0  167    2    0    8  641    0    6\n",
      "    44    0]\n",
      " [   1    1    0    1    0    0    0    4    0    0    0    0 1791    0\n",
      "     2    0]\n",
      " [   0   11    1    0    0    0    0    0    0    0    0    0    0  481\n",
      "     0    0]\n",
      " [  24   40    0   13    3    4    0   38    1    0   12    9    2    1\n",
      "  1873    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n",
      "==================================================\n",
      "\n",
      "========== Evaluation on Final Test Set ==========\n",
      "Overall Tagging Accuracy: 0.910072126451481\n",
      "\n",
      "Detailed Metrics (Precision, Recall, F1-Score) per UPOS Tag:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ     0.8593    0.7660    0.8099      1363\n",
      "         ADP     0.8940    0.9885    0.9389      2782\n",
      "         ADV     0.8963    0.8094    0.8506       299\n",
      "         AUX     0.9122    0.9680    0.9393       719\n",
      "       CCONJ     0.9498    0.9988    0.9737       815\n",
      "         DET     0.7375    0.9300    0.8226       414\n",
      "        INTJ     0.8571    0.4000    0.5455        15\n",
      "        NOUN     0.9463    0.8866    0.9154      6752\n",
      "         NUM     0.8849    0.8643    0.8745       258\n",
      "        PART     1.0000    0.9091    0.9524        22\n",
      "        PRON     0.8556    0.9504    0.9006       686\n",
      "       PROPN     0.7511    0.6717    0.7092      1002\n",
      "       PUNCT     0.9977    0.9994    0.9986      1767\n",
      "       SCONJ     0.9173    0.9608    0.9386       485\n",
      "        VERB     0.8972    0.9534    0.9245      2169\n",
      "           X     0.0000    0.0000    0.0000         1\n",
      "\n",
      "    accuracy                         0.9101     19549\n",
      "   macro avg     0.8348    0.8160    0.8184     19549\n",
      "weighted avg     0.9109    0.9101    0.9089     19549\n",
      "\n",
      "\n",
      "Confusion Matrix (Labels Order):\n",
      "Labels: ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'VERB', 'X']\n",
      "[[1044   69   11    8   11   16    0  110    1    0    1   43    1    6\n",
      "    42    0]\n",
      " [   2 2750    0    0    0    0    0   20    1    0    1    2    0    6\n",
      "     0    0]\n",
      " [   9   10  242    2    2    2    0   10    0    0   14    2    0    3\n",
      "     3    0]\n",
      " [   0    1    0  696    0    2    0    3    1    0    1    2    0    1\n",
      "    12    0]\n",
      " [   0    0    1    0  814    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   1    0    1    0    0  385    0   12    0    0   15    0    0    0\n",
      "     0    0]\n",
      " [   1    3    0    0    1    1    6    0    0    0    2    0    0    0\n",
      "     1    0]\n",
      " [ 107  156   10   34   18   74    0 5986   22    0   46  157    2   21\n",
      "   119    0]\n",
      " [   0    5    0    1    0    2    0   17  223    0    0    9    0    0\n",
      "     1    0]\n",
      " [   0    0    0    0    0    0    0    0    0   20    0    0    1    1\n",
      "     0    0]\n",
      " [   0    0    2    1    0   24    0    5    0    0  652    0    0    1\n",
      "     1    0]\n",
      " [  34   46    2    6    7   12    0  141    4    0   19  673    0    1\n",
      "    57    0]\n",
      " [   0    0    0    0    0    0    0    1    0    0    0    0 1766    0\n",
      "     0    0]\n",
      " [   0   13    0    0    0    0    1    4    0    0    0    0    0  466\n",
      "     1    0]\n",
      " [  16   23    1   15    4    4    0   17    0    0   11    8    0    2\n",
      "  2068    0]\n",
      " [   1    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n",
      "==================================================\n",
      "\n",
      "========== Evaluation on Final Test Set ==========\n",
      "Overall Tagging Accuracy: 0.910072126451481\n",
      "\n",
      "Detailed Metrics (Precision, Recall, F1-Score) per UPOS Tag:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ     0.8593    0.7660    0.8099      1363\n",
      "         ADP     0.8940    0.9885    0.9389      2782\n",
      "         ADV     0.8963    0.8094    0.8506       299\n",
      "         AUX     0.9122    0.9680    0.9393       719\n",
      "       CCONJ     0.9498    0.9988    0.9737       815\n",
      "         DET     0.7375    0.9300    0.8226       414\n",
      "        INTJ     0.8571    0.4000    0.5455        15\n",
      "        NOUN     0.9463    0.8866    0.9154      6752\n",
      "         NUM     0.8849    0.8643    0.8745       258\n",
      "        PART     1.0000    0.9091    0.9524        22\n",
      "        PRON     0.8556    0.9504    0.9006       686\n",
      "       PROPN     0.7511    0.6717    0.7092      1002\n",
      "       PUNCT     0.9977    0.9994    0.9986      1767\n",
      "       SCONJ     0.9173    0.9608    0.9386       485\n",
      "        VERB     0.8972    0.9534    0.9245      2169\n",
      "           X     0.0000    0.0000    0.0000         1\n",
      "\n",
      "    accuracy                         0.9101     19549\n",
      "   macro avg     0.8348    0.8160    0.8184     19549\n",
      "weighted avg     0.9109    0.9101    0.9089     19549\n",
      "\n",
      "\n",
      "Confusion Matrix (Labels Order):\n",
      "Labels: ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'VERB', 'X']\n",
      "[[1044   69   11    8   11   16    0  110    1    0    1   43    1    6\n",
      "    42    0]\n",
      " [   2 2750    0    0    0    0    0   20    1    0    1    2    0    6\n",
      "     0    0]\n",
      " [   9   10  242    2    2    2    0   10    0    0   14    2    0    3\n",
      "     3    0]\n",
      " [   0    1    0  696    0    2    0    3    1    0    1    2    0    1\n",
      "    12    0]\n",
      " [   0    0    1    0  814    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   1    0    1    0    0  385    0   12    0    0   15    0    0    0\n",
      "     0    0]\n",
      " [   1    3    0    0    1    1    6    0    0    0    2    0    0    0\n",
      "     1    0]\n",
      " [ 107  156   10   34   18   74    0 5986   22    0   46  157    2   21\n",
      "   119    0]\n",
      " [   0    5    0    1    0    2    0   17  223    0    0    9    0    0\n",
      "     1    0]\n",
      " [   0    0    0    0    0    0    0    0    0   20    0    0    1    1\n",
      "     0    0]\n",
      " [   0    0    2    1    0   24    0    5    0    0  652    0    0    1\n",
      "     1    0]\n",
      " [  34   46    2    6    7   12    0  141    4    0   19  673    0    1\n",
      "    57    0]\n",
      " [   0    0    0    0    0    0    0    1    0    0    0    0 1766    0\n",
      "     0    0]\n",
      " [   0   13    0    0    0    0    1    4    0    0    0    0    0  466\n",
      "     1    0]\n",
      " [  16   23    1   15    4    4    0   17    0    0   11    8    0    2\n",
      "  2068    0]\n",
      " [   1    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pyconll\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "TRAIN_PATH = 'PosData/fa_perdt-ud-train.conllu'\n",
    "DEV_PATH = 'PosData/fa_perdt-ud-dev.conllu'\n",
    "TEST_PATH = 'PosData/fa_perdt-ud-test.conllu'\n",
    "\n",
    "def load_conllu(path):\n",
    "    \"\"\"apparently this is best practice to load conllu files using pyconll\"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        conll = pyconll.load_from_file(path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {path}. Returning empty data.\")\n",
    "        return []\n",
    "\n",
    "    for sentence in conll:\n",
    "        words = [token.form for token in sentence]\n",
    "        tags = [token.upos for token in sentence]\n",
    "        # Skip sentences with inconsistent tokenization/annotation\n",
    "        if all(words) and all(tags) and len(words) == len(tags):\n",
    "            data.append((words, tags))\n",
    "    return data\n",
    "\n",
    "# define train dev and test data\n",
    "print(\"Loading data...\")\n",
    "train_data = load_conllu(TRAIN_PATH)\n",
    "dev_data = load_conllu(DEV_PATH)\n",
    "test_data = load_conllu(TEST_PATH)\n",
    "print(f\"Train sentences: {len(train_data)}, Dev sentences: {len(dev_data)}, Test sentences: {len(test_data)}\")\n",
    "\n",
    "# --- HMM Training Function ---\n",
    "def train_hmm(sentences, alpha=1.0, unk_threshold=1):\n",
    "    \"\"\"Trains the HMM by calculating log probabilities with Laplace smoothing.\"\"\"\n",
    "    tag_counts = Counter()\n",
    "    emission_counts = defaultdict(Counter)\n",
    "    transition_counts = defaultdict(Counter)\n",
    "    word_counts = Counter()\n",
    "\n",
    "    for words, tags in sentences:\n",
    "        # Check if the sentence is not empty\n",
    "        if not tags:\n",
    "            continue\n",
    "            \n",
    "        # Initial Transition (<s> -> Tag_1)\n",
    "        prev = '<s>'\n",
    "        transition_counts['<s>'][tags[0]] += 1\n",
    "        \n",
    "        # Internal Transitions and Emissions\n",
    "        for w, t in zip(words, tags):\n",
    "            word_counts[w] += 1\n",
    "            tag_counts[t] += 1\n",
    "            emission_counts[t][w] += 1\n",
    "            transition_counts[prev][t] += 1\n",
    "            prev = t\n",
    "            \n",
    "        # Final Transition (Tag_n -> </s>)\n",
    "        transition_counts[prev]['</s>'] += 1\n",
    "\n",
    "    # Build vocab with UNK handling for low-frequency words\n",
    "    vocab = set(w for w,c in word_counts.items() if c > unk_threshold)\n",
    "    vocab.add('<UNK>')\n",
    "    tags = list(tag_counts.keys())\n",
    "\n",
    "    # Precompute log-probabilities with Laplace smoothing\n",
    "    V = len(vocab)\n",
    "    tag_total = {t: tag_counts[t] for t in tags}\n",
    "\n",
    "    # Emission probabilities (B)\n",
    "    emission_logprob = defaultdict(dict)\n",
    "    for t in tags:\n",
    "        denom = tag_total[t] + alpha * V\n",
    "        for w in vocab:\n",
    "            # Check for word in the original counts\n",
    "            count = emission_counts[t].get(w, 0)\n",
    "            emission_logprob[t][w] = math.log((count + alpha) / denom)\n",
    "    \n",
    "    # Transition probabilities (A)\n",
    "    transition_logprob = defaultdict(dict)\n",
    "    all_possible_prev_tags = list(transition_counts.keys())\n",
    "    \n",
    "    for prev in all_possible_prev_tags:\n",
    "        total = sum(transition_counts[prev].values())\n",
    "        \n",
    "        # N includes all regular tags + the end-of-sentence tag </s>\n",
    "        N = len(tags) + (1 if prev != '<s>' else 0) # Only count </s> if not starting \n",
    "        \n",
    "        # Use tags + </s> as all possible next states for smoothing\n",
    "        possible_next_states = tags + (['</s>'] if prev != '<s>' else []) \n",
    "\n",
    "        # Correct calculation of N for smoothing denominator\n",
    "        N_smoothing = len(tags) + 1 # Use |Tags| + 1 for start tag smoothing\n",
    "        if prev != '<s>':\n",
    "             N_smoothing = len(tags) + 1 # Use |Tags| + 1 for regular tag smoothing\n",
    "\n",
    "        for t in possible_next_states:\n",
    "            c = transition_counts[prev].get(t, 0)\n",
    "            transition_logprob[prev][t] = math.log((c + alpha) / (total + alpha * N_smoothing))\n",
    "\n",
    "    model = {\n",
    "        'tags': tags,\n",
    "        'vocab': vocab,\n",
    "        'emission_logprob': emission_logprob,\n",
    "        'transition_logprob': transition_logprob,\n",
    "        'tag_counts': tag_total,\n",
    "        'alpha': alpha\n",
    "    }\n",
    "    return model\n",
    "\n",
    "# --- Viterbi Decoding Function ---\n",
    "def viterbi(model, words):\n",
    "    \"\"\"Uses the Viterbi algorithm to find the most likely sequence of tags.\"\"\"\n",
    "    tags = model['tags']\n",
    "    emission = model['emission_logprob']\n",
    "    trans = model['transition_logprob']\n",
    "    vocab = model['vocab']\n",
    "\n",
    "    W = len(words)\n",
    "    if W == 0:\n",
    "        return []\n",
    "        \n",
    "    # Map rare/unseen words to <UNK>\n",
    "    obs = [w.lower() if w in vocab else '<UNK>' for w in words]\n",
    "\n",
    "    V = [{} for _ in range(W)] # Viterbi probabilities (log)\n",
    "    backpointer = [{} for _ in range(W)]\n",
    "\n",
    "    # Initialization (t=0)\n",
    "    for t in tags:\n",
    "        # Transition from <s>\n",
    "        tp = trans.get('<s>', {}).get(t, math.log(1e-12))\n",
    "        # Emission for obs[0] from tag t\n",
    "        ep = emission.get(t, {}).get(obs[0], emission.get(t, {}).get('<UNK>', math.log(1e-12)))\n",
    "        V[0][t] = tp + ep\n",
    "        backpointer[0][t] = None\n",
    "\n",
    "    # Recursion (t > 0)\n",
    "    for i in range(1, W):\n",
    "        for t in tags:\n",
    "            max_prob = -np.inf\n",
    "            arg_best = None\n",
    "            # Emission for obs[i] from tag t\n",
    "            ep = emission.get(t, {}).get(obs[i], emission.get(t, {}).get('<UNK>', math.log(1e-12)))\n",
    "            \n",
    "            for t_prev in tags:\n",
    "                # Transition from t_prev to t\n",
    "                tp = trans.get(t_prev, {}).get(t, math.log(1e-12))\n",
    "                \n",
    "                prob = V[i-1][t_prev] + tp + ep\n",
    "                \n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    arg_best = t_prev\n",
    "            \n",
    "            V[i][t] = max_prob\n",
    "            backpointer[i][t] = arg_best\n",
    "\n",
    "    # Termination: Find best tag at the last position (W-1)\n",
    "    best_prob = -np.inf\n",
    "    best_tag = None\n",
    "    for t in tags:\n",
    "        # Include transition to </s>\n",
    "        prob = V[W-1][t] + trans.get(t, {}).get('</s>', math.log(1e-12))\n",
    "        if prob > best_prob:\n",
    "            best_prob = prob\n",
    "            best_tag = t\n",
    "\n",
    "    # Backtrack\n",
    "    out_tags = [best_tag] * W\n",
    "    curr_tag = best_tag\n",
    "    for i in range(W-1, 0, -1):\n",
    "        curr_tag = backpointer[i][curr_tag]\n",
    "        out_tags[i-1] = curr_tag\n",
    "        \n",
    "    return out_tags\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "def evaluate_model(model, dataset, dataset_name='set'):\n",
    "    \"\"\"Performs tagging and prints metrics (Accuracy, Confusion Matrix, Classification Report).\"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    # Get all unique tags from the model to ensure all tags are included in the report\n",
    "    all_model_tags = sorted(model['tags']) \n",
    "    \n",
    "    for words, tags in dataset:\n",
    "        preds = viterbi(model, words)\n",
    "        # Ensure predictions and true tags have the same length before extending\n",
    "        if len(preds) == len(tags):\n",
    "            y_true.extend(tags)\n",
    "            y_pred.extend(preds)\n",
    "\n",
    "    print(f'\\n{\"=\"*10} Evaluation on {dataset_name} Set {\"=\"*10}')\n",
    "    if not y_true:\n",
    "        print(\"No valid data for evaluation.\")\n",
    "        return\n",
    "\n",
    "    # Accuracy\n",
    "    print('Overall Tagging Accuracy:', accuracy_score(y_true, y_pred))\n",
    "    \n",
    "    # Classification Report (Precision, Recall, F1-Score)\n",
    "    print('\\nDetailed Metrics (Precision, Recall, F1-Score) per UPOS Tag:')\n",
    "    # Use only tags present in the true labels for the report\n",
    "    target_names = sorted(list(set(y_true)))\n",
    "    print(classification_report(y_true, y_pred, labels=target_names, target_names=target_names, digits=4, zero_division=0))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    print('\\nConfusion Matrix (Labels Order):')\n",
    "    # Use all model tags for consistent matrix labels\n",
    "    labels = all_model_tags\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    print('Labels:', labels)\n",
    "    print(cm)\n",
    "    print('='*50)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training HMM model...\")\n",
    "model = train_hmm(train_data)\n",
    "print(\"HMM Training complete.\")\n",
    "\n",
    "# 1. Evaluate on Development Set (Tuning)\n",
    "evaluate_model(model, dev_data, 'Development (Dev)')\n",
    "\n",
    "# 2. Evaluate on Test Set (Final Score)\n",
    "evaluate_model(model, test_data, 'Final Test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
