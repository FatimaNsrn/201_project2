{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkKbAuMjPuPL",
        "outputId": "8995db6f-2665-4cec-acdb-0d58e22ea9fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.12/dist-packages (from seqeval) (1.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=c3466bed497384eed88127527ca5066ff9652293f242f116ed03dac24e98a086\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/b8/73/0b2c1a76b701a677653dd79ece07cfabd7457989dbfbdcd8d7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets seqeval torch tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.vec.gz\n",
        "!gunzip cc.fa.300.vec.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1O8oBbWKRY2g",
        "outputId": "91baf3e0-1af3-4c9e-ee95-3f7f9e05ae49"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-22 11:14:22--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.154.144.13, 18.154.144.87, 18.154.144.102, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.154.144.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1258183862 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.fa.300.vec.gz’\n",
            "\n",
            "cc.fa.300.vec.gz    100%[===================>]   1.17G   124MB/s    in 9.5s    \n",
            "\n",
            "2025-11-22 11:14:32 (126 MB/s) - ‘cc.fa.300.vec.gz’ saved [1258183862/1258183862]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# 1. Imports\n",
        "# -------------------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from seqeval.metrics import classification_report, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Set seeds\n",
        "# -------------------------------\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "set_seed()\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Load PEYMA dataset\n",
        "# -------------------------------\n",
        "dataset = load_dataset(\"AliFartout/PEYMA-ARMAN-Mixed\")\n",
        "train_data = dataset[\"train\"]\n",
        "val_data   = dataset[\"validation\"]\n",
        "test_data  = dataset[\"test\"]\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Label normalization to IOB2 format\n",
        "# -------------------------------\n",
        "def normalize_tags(tags):\n",
        "    \"\"\"Convert tags to IOB2 format (B-ORG, I-ORG) for seqeval compatibility\"\"\"\n",
        "    return [t.replace(\"_\", \"-\") for t in tags]\n",
        "\n",
        "# CRITICAL: Normalize and collect labels at the same time\n",
        "all_words, all_chars, all_labels = set(), set(), set()\n",
        "normalized_data = {\"train\": [], \"validation\": [], \"test\": []}\n",
        "\n",
        "for split_name, split in [(\"train\", train_data), (\"validation\", val_data), (\"test\", test_data)]:\n",
        "    for sample in split:\n",
        "        tokens = sample[\"tokens\"]\n",
        "        normalized_tags = normalize_tags(sample[\"ner_tags_names\"])\n",
        "\n",
        "        # Store normalized version\n",
        "        normalized_data[split_name].append({\n",
        "            \"tokens\": tokens,\n",
        "            \"ner_tags_names\": normalized_tags\n",
        "        })\n",
        "\n",
        "        # Collect vocabulary\n",
        "        for w, t in zip(tokens, normalized_tags):\n",
        "            all_words.add(w)\n",
        "            all_labels.add(t)\n",
        "            all_chars.update(list(w))\n",
        "\n",
        "# Replace original data with normalized data\n",
        "train_data = normalized_data[\"train\"]\n",
        "val_data = normalized_data[\"validation\"]\n",
        "test_data = normalized_data[\"test\"]\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Build vocabularies\n",
        "# -------------------------------\n",
        "\n",
        "word2id = {\"<PAD>\":0, \"<UNK>\":1}\n",
        "for w in all_words:\n",
        "    word2id[w] = len(word2id)\n",
        "\n",
        "char2id = {\"<PAD>\":0, \"<UNK>\":1}\n",
        "for ch in all_chars:\n",
        "    char2id[ch] = len(char2id)\n",
        "\n",
        "# Build label encoder AFTER normalization\n",
        "# Ensure \"O\" label is included\n",
        "all_labels.add(\"O\")  # Critical: add O label if missing\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(sorted(list(all_labels)))  # Sort for consistency\n",
        "\n",
        "# Create mapping for quick inverse transform - NOW CORRECT\n",
        "id2label = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
        "print(f\"Sample labels: {list(id2label.values())[:10]}\")  # Verify format\n",
        "print(f\"Total labels: {len(id2label)}\")\n",
        "print(f\"Has O label: {'O' in id2label.values()}\")\n",
        "print(f\"Label distribution in training: {Counter([l for sample in train_data for l in sample['ner_tags_names']]).most_common(5)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Load FastText embeddings\n",
        "# -------------------------------\n",
        "EMB_DIM = 300\n",
        "fasttext_path = \"/content/cc.fa.300.vec\"\n",
        "\n",
        "print(\"Loading FastText vectors...\")\n",
        "fasttext = {}\n",
        "with open(fasttext_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "    next(f)  # skip header\n",
        "    for line in tqdm(f, total=2000000):\n",
        "        parts = line.rstrip().split(\" \")\n",
        "        if len(parts) < EMB_DIM + 1:\n",
        "            continue\n",
        "        fasttext[parts[0]] = np.asarray(parts[1:], dtype=\"float32\")\n",
        "\n",
        "embedding_matrix = np.zeros((len(word2id), EMB_DIM), dtype=\"float32\")\n",
        "oov_count = 0\n",
        "for word, idx in word2id.items():\n",
        "    if word in fasttext:\n",
        "        embedding_matrix[idx] = fasttext[word]\n",
        "    else:\n",
        "        embedding_matrix[idx] = np.random.normal(scale=0.1, size=(EMB_DIM,))\n",
        "        oov_count += 1\n",
        "print(f\"OOV words: {oov_count} / {len(word2id)}\")\n",
        "embedding_matrix = torch.tensor(embedding_matrix)\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Dataset + batching\n",
        "# -------------------------------\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, split):\n",
        "        self.data = split\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self.data[idx][\"tokens\"]\n",
        "        labels = self.data[idx][\"ner_tags_names\"]\n",
        "\n",
        "        word_ids = torch.tensor([word2id.get(w, 1) for w in tokens])\n",
        "        char_ids = [torch.tensor([char2id.get(c, 1) for c in w]) for w in tokens]\n",
        "        label_ids = torch.tensor(label_encoder.transform(labels))\n",
        "\n",
        "        return word_ids, char_ids, label_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "def pad_batch(batch):\n",
        "    word_seqs, char_seqs, label_seqs = zip(*batch)\n",
        "    max_len = max(len(w) for w in word_seqs)\n",
        "\n",
        "    padded_words, padded_labels, padded_chars, masks = [], [], [], []\n",
        "\n",
        "    # Use -100 as padding label (standard in PyTorch for ignoring in loss)\n",
        "    PAD_LABEL = -100\n",
        "\n",
        "    for w, cseq, l in zip(word_seqs, char_seqs, label_seqs):\n",
        "        pad_len = max_len - len(w)\n",
        "        padded_words.append(torch.cat([w, torch.zeros(pad_len, dtype=torch.long)]))\n",
        "        padded_labels.append(torch.cat([l, torch.full((pad_len,), PAD_LABEL, dtype=torch.long)]))\n",
        "\n",
        "        # Create mask: 1 for real tokens, 0 for padding\n",
        "        mask = torch.cat([torch.ones(len(w)), torch.zeros(pad_len)])\n",
        "        masks.append(mask)\n",
        "\n",
        "        cseq = cseq + [torch.zeros(1, dtype=torch.long)] * pad_len\n",
        "        padded_chars.append(cseq)\n",
        "\n",
        "    max_char_len = max(len(c) for seq in padded_chars for c in seq)\n",
        "    final_chars = []\n",
        "    for seq in padded_chars:\n",
        "        padded = []\n",
        "        for c in seq:\n",
        "            pad_len = max_char_len - len(c)\n",
        "            padded.append(torch.cat([c, torch.zeros(pad_len, dtype=torch.long)]))\n",
        "        final_chars.append(torch.stack(padded))\n",
        "\n",
        "    return (torch.stack(padded_words),\n",
        "            torch.stack(final_chars),\n",
        "            torch.stack(padded_labels),\n",
        "            torch.stack(masks))\n",
        "\n",
        "train_loader = DataLoader(NERDataset(train_data), batch_size=16, shuffle=True, collate_fn=pad_batch)\n",
        "val_loader   = DataLoader(NERDataset(val_data), batch_size=16, collate_fn=pad_batch)\n",
        "test_loader  = DataLoader(NERDataset(test_data), batch_size=16, collate_fn=pad_batch)\n",
        "\n",
        "# -------------------------------\n",
        "# 8. Improved CNN + BiLSTM model\n",
        "# -------------------------------\n",
        "class CNN_BiLSTM_NER(nn.Module):\n",
        "    def __init__(self, embedding_matrix, char_vocab, num_labels, dropout=0.5):\n",
        "        super().__init__()\n",
        "        vocab_size, emb_dim = embedding_matrix.shape\n",
        "\n",
        "        # Unfreeze embeddings to allow fine-tuning\n",
        "        self.word_emb = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "        self.word_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Character-level CNN\n",
        "        self.char_emb = nn.Embedding(char_vocab, 30, padding_idx=0)\n",
        "        self.char_cnn = nn.Conv1d(30, 100, kernel_size=3, padding=1)\n",
        "        self.char_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # 2-layer BiLSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=emb_dim + 100,\n",
        "            hidden_size=256,\n",
        "            num_layers=2,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(512, num_labels)\n",
        "\n",
        "    def forward(self, words, chars):\n",
        "        # Word embeddings\n",
        "        word_embed = self.word_dropout(self.word_emb(words))\n",
        "\n",
        "        # Character embeddings and CNN\n",
        "        B, L, C = chars.shape\n",
        "        chars = chars.long().view(B*L, C)\n",
        "        char_emb = self.char_emb(chars).transpose(1, 2)\n",
        "        char_feat = torch.max(torch.relu(self.char_cnn(char_emb)), dim=2).values\n",
        "        char_feat = self.char_dropout(char_feat.view(B, L, 100))\n",
        "\n",
        "        # Combine word and character features\n",
        "        combined = torch.cat([word_embed, char_feat], dim=-1)\n",
        "\n",
        "        # BiLSTM\n",
        "        lstm_out, _ = self.lstm(combined)\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "\n",
        "        return self.fc(lstm_out)\n",
        "\n",
        "# -------------------------------\n",
        "# 9. Prepare weighted loss\n",
        "# -------------------------------\n",
        "# Calculate class weights for imbalanced data\n",
        "all_train_labels = []\n",
        "for sample in train_data:\n",
        "    all_train_labels.extend(sample[\"ner_tags_names\"])\n",
        "\n",
        "label_counts = Counter(all_train_labels)\n",
        "total = sum(label_counts.values())\n",
        "\n",
        "# Calculate inverse frequency weights\n",
        "weights = []\n",
        "for label in label_encoder.classes_:\n",
        "    if label in label_counts:\n",
        "        # Use square root of inverse frequency for smoother weighting\n",
        "        weight = (total / label_counts[label]) ** 0.5\n",
        "    else:\n",
        "        weight = 1.0\n",
        "    weights.append(weight)\n",
        "\n",
        "weights_tensor = torch.tensor(weights, dtype=torch.float)\n",
        "\n",
        "# Normalize weights so O label isn't too dominant\n",
        "max_weight = max(weights)\n",
        "weights_tensor = weights_tensor / max_weight\n",
        "\n",
        "print(f\"Label weights (first 10): {weights_tensor[:10].tolist()}\")\n",
        "print(f\"O label weight: {weights_tensor[list(label_encoder.classes_).index('O') if 'O' in label_encoder.classes_ else -1]}\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = CNN_BiLSTM_NER(embedding_matrix, len(char2id), len(label_encoder.classes_)).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=weights_tensor.to(device), ignore_index=-100)  # Ignore padding\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
        "\n",
        "# -------------------------------\n",
        "# 10. Helper function to convert predictions to labels\n",
        "# -------------------------------\n",
        "def convert_to_labels(predictions, labels):\n",
        "    \"\"\"Convert tensor predictions to label lists, filtering padding\"\"\"\n",
        "    pred_labels = []\n",
        "    true_labels = []\n",
        "\n",
        "    for pred_seq, true_seq in zip(predictions, labels):\n",
        "        pred_seq = pred_seq.cpu().tolist()\n",
        "        true_seq = true_seq.cpu().tolist()\n",
        "\n",
        "        # Filter out padding (label -100)\n",
        "        pred_tags = []\n",
        "        true_tags = []\n",
        "        for p, t in zip(pred_seq, true_seq):\n",
        "            if t == -100:  # Skip padding\n",
        "                continue\n",
        "            pred_tags.append(id2label[p])\n",
        "            true_tags.append(id2label[t])\n",
        "\n",
        "        if pred_tags:  # Only add non-empty sequences\n",
        "            pred_labels.append(pred_tags)\n",
        "            true_labels.append(true_tags)\n",
        "\n",
        "    return pred_labels, true_labels\n",
        "\n",
        "# -------------------------------\n",
        "# 11. Training with validation F1\n",
        "# -------------------------------\n",
        "best_val_f1 = 0\n",
        "patience_counter = 0\n",
        "patience = 5\n",
        "num_epochs = 20\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\\n\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for words, chars, labels, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
        "        words, chars, labels = words.to(device), chars.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(words, chars)\n",
        "        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    y_val_true, y_val_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for words, chars, labels, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
        "            words, chars, labels = words.to(device), chars.to(device), labels.to(device)\n",
        "            logits = model(words, chars)\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            pred = logits.argmax(-1)\n",
        "            pred_labels, true_labels = convert_to_labels(pred, labels)\n",
        "            y_val_pred.extend(pred_labels)\n",
        "            y_val_true.extend(true_labels)\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_f1 = f1_score(y_val_true, y_val_pred, average=\"micro\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f}\")\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(val_f1)\n",
        "\n",
        "    # Early stopping & checkpoint\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "        patience_counter = 0\n",
        "        print(f\"  → New best model saved! (F1: {val_f1:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs!\")\n",
        "            break\n",
        "\n",
        "print(f\"\\nTraining completed. Best validation F1: {best_val_f1:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 12. Test evaluation\n",
        "# -------------------------------\n",
        "print(\"\\nLoading best model for test evaluation...\")\n",
        "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
        "model.eval()\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for words, chars, labels, masks in tqdm(test_loader, desc=\"Testing\"):\n",
        "        words, chars, labels = words.to(device), chars.to(device), labels.to(device)\n",
        "        logits = model(words, chars)\n",
        "        pred = logits.argmax(-1)\n",
        "\n",
        "        pred_labels, true_labels = convert_to_labels(pred, labels)\n",
        "        y_pred.extend(pred_labels)\n",
        "        y_true.extend(true_labels)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL TEST RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, digits=4))\n",
        "print(\"\\nAggregate Scores:\")\n",
        "print(f\"  Micro F1:    {f1_score(y_true, y_pred, average='micro'):.4f}\")\n",
        "print(f\"  Macro F1:    {f1_score(y_true, y_pred, average='macro'):.4f}\")\n",
        "print(f\"  Weighted F1: {f1_score(y_true, y_pred, average='weighted'):.4f}\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnRerwmWmMpj",
        "outputId": "1c4ef52e-0fd1-4abc-8226-ff440fb72a5a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample labels: [np.str_('B-DAT'), np.str_('B-EVE'), np.str_('B-FAC'), np.str_('B-LOC'), np.str_('B-MON'), np.str_('B-ORG'), np.str_('B-PCT'), np.str_('B-PER'), np.str_('B-PRO'), np.str_('B-TIM')]\n",
            "Total labels: 21\n",
            "Has O label: True\n",
            "Label distribution in training: [('O', 745941), ('I-ORG', 21230), ('B-ORG', 15719), ('B-LOC', 12976), ('B-PER', 11335)]\n",
            "Loading FastText vectors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000000/2000000 [02:40<00:00, 12426.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOV words: 7493 / 29408\n",
            "Label weights (first 10): [0.3843235373497009, 0.4012308716773987, 0.40794211626052856, 0.13079950213432312, 0.7055195569992065, 0.11884038150310516, 0.9135570526123047, 0.13994769752025604, 0.3593672811985016, 1.0]\n",
            "O label weight: 0.01725139655172825\n",
            "Using device: cuda\n",
            "\n",
            "Starting training...\n",
            "Total parameters: 11,770,777\n",
            "Trainable parameters: 11,770,777\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 [Train]: 100%|██████████| 1649/1649 [01:04<00:00, 25.38it/s]\n",
            "Epoch 1/20 [Val]: 100%|██████████| 206/206 [00:07<00:00, 26.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train Loss: 0.7104 | Val Loss: 0.3488 | Val F1: 0.6298\n",
            "  → New best model saved! (F1: 0.6298)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20 [Train]: 100%|██████████| 1649/1649 [01:04<00:00, 25.57it/s]\n",
            "Epoch 2/20 [Val]: 100%|██████████| 206/206 [00:06<00:00, 30.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 02 | Train Loss: 0.2648 | Val Loss: 0.2449 | Val F1: 0.6955\n",
            "  → New best model saved! (F1: 0.6955)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20 [Train]: 100%|██████████| 1649/1649 [01:04<00:00, 25.67it/s]\n",
            "Epoch 3/20 [Val]: 100%|██████████| 206/206 [00:05<00:00, 34.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 03 | Train Loss: 0.1637 | Val Loss: 0.2051 | Val F1: 0.7263\n",
            "  → New best model saved! (F1: 0.7263)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20 [Train]: 100%|██████████| 1649/1649 [01:05<00:00, 25.19it/s]\n",
            "Epoch 4/20 [Val]: 100%|██████████| 206/206 [00:05<00:00, 38.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 04 | Train Loss: 0.1148 | Val Loss: 0.1830 | Val F1: 0.7906\n",
            "  → New best model saved! (F1: 0.7906)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20 [Train]: 100%|██████████| 1649/1649 [01:05<00:00, 25.03it/s]\n",
            "Epoch 5/20 [Val]: 100%|██████████| 206/206 [00:05<00:00, 36.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 05 | Train Loss: 0.0887 | Val Loss: 0.1968 | Val F1: 0.7867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20 [Train]: 100%|██████████| 1649/1649 [01:06<00:00, 24.97it/s]\n",
            "Epoch 6/20 [Val]: 100%|██████████| 206/206 [00:06<00:00, 30.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 06 | Train Loss: 0.0713 | Val Loss: 0.1775 | Val F1: 0.8199\n",
            "  → New best model saved! (F1: 0.8199)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20 [Train]: 100%|██████████| 1649/1649 [01:04<00:00, 25.64it/s]\n",
            "Epoch 7/20 [Val]: 100%|██████████| 206/206 [00:07<00:00, 29.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 07 | Train Loss: 0.0514 | Val Loss: 0.1885 | Val F1: 0.8348\n",
            "  → New best model saved! (F1: 0.8348)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20 [Train]: 100%|██████████| 1649/1649 [01:04<00:00, 25.65it/s]\n",
            "Epoch 8/20 [Val]: 100%|██████████| 206/206 [00:06<00:00, 33.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 08 | Train Loss: 0.0476 | Val Loss: 0.1788 | Val F1: 0.8236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20 [Train]: 100%|██████████| 1649/1649 [01:04<00:00, 25.49it/s]\n",
            "Epoch 9/20 [Val]: 100%|██████████| 206/206 [00:05<00:00, 39.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 09 | Train Loss: 0.0402 | Val Loss: 0.1832 | Val F1: 0.8410\n",
            "  → New best model saved! (F1: 0.8410)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20 [Train]: 100%|██████████| 1649/1649 [01:06<00:00, 24.89it/s]\n",
            "Epoch 10/20 [Val]: 100%|██████████| 206/206 [00:05<00:00, 37.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Train Loss: 0.0346 | Val Loss: 0.2008 | Val F1: 0.8561\n",
            "  → New best model saved! (F1: 0.8561)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20 [Train]: 100%|██████████| 1649/1649 [01:05<00:00, 25.33it/s]\n",
            "Epoch 11/20 [Val]: 100%|██████████| 206/206 [00:06<00:00, 30.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 | Train Loss: 0.0297 | Val Loss: 0.2235 | Val F1: 0.8567\n",
            "  → New best model saved! (F1: 0.8567)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20 [Train]: 100%|██████████| 1649/1649 [01:04<00:00, 25.59it/s]\n",
            "Epoch 12/20 [Val]: 100%|██████████| 206/206 [00:07<00:00, 28.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 | Train Loss: 0.0290 | Val Loss: 0.2354 | Val F1: 0.8632\n",
            "  → New best model saved! (F1: 0.8632)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20 [Train]: 100%|██████████| 1649/1649 [01:04<00:00, 25.52it/s]\n",
            "Epoch 13/20 [Val]: 100%|██████████| 206/206 [00:06<00:00, 32.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 | Train Loss: 0.0253 | Val Loss: 0.2376 | Val F1: 0.8471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20 [Train]: 100%|██████████| 1649/1649 [01:04<00:00, 25.48it/s]\n",
            "Epoch 14/20 [Val]: 100%|██████████| 206/206 [00:05<00:00, 37.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 | Train Loss: 0.0233 | Val Loss: 0.2661 | Val F1: 0.8391\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20 [Train]: 100%|██████████| 1649/1649 [01:06<00:00, 24.89it/s]\n",
            "Epoch 15/20 [Val]: 100%|██████████| 206/206 [00:05<00:00, 38.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 | Train Loss: 0.0216 | Val Loss: 0.2606 | Val F1: 0.8521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20 [Train]: 100%|██████████| 1649/1649 [01:05<00:00, 25.29it/s]\n",
            "Epoch 16/20 [Val]: 100%|██████████| 206/206 [00:06<00:00, 32.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 | Train Loss: 0.0142 | Val Loss: 0.2487 | Val F1: 0.8653\n",
            "  → New best model saved! (F1: 0.8653)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20 [Train]: 100%|██████████| 1649/1649 [01:04<00:00, 25.38it/s]\n",
            "Epoch 17/20 [Val]: 100%|██████████| 206/206 [00:07<00:00, 28.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 | Train Loss: 0.0110 | Val Loss: 0.2669 | Val F1: 0.8640\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20 [Train]: 100%|██████████| 1649/1649 [01:04<00:00, 25.40it/s]\n",
            "Epoch 18/20 [Val]: 100%|██████████| 206/206 [00:07<00:00, 28.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 | Train Loss: 0.0087 | Val Loss: 0.2812 | Val F1: 0.8792\n",
            "  → New best model saved! (F1: 0.8792)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20 [Train]: 100%|██████████| 1649/1649 [01:04<00:00, 25.47it/s]\n",
            "Epoch 19/20 [Val]: 100%|██████████| 206/206 [00:06<00:00, 32.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 | Train Loss: 0.0076 | Val Loss: 0.2791 | Val F1: 0.8671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20 [Train]: 100%|██████████| 1649/1649 [01:04<00:00, 25.38it/s]\n",
            "Epoch 20/20 [Val]: 100%|██████████| 206/206 [00:05<00:00, 38.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 | Train Loss: 0.0076 | Val Loss: 0.2877 | Val F1: 0.8730\n",
            "\n",
            "Training completed. Best validation F1: 0.8792\n",
            "\n",
            "Loading best model for test evaluation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 206/206 [00:07<00:00, 27.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "FINAL TEST RESULTS\n",
            "================================================================================\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         DAT     0.5425    0.7486    0.6291       179\n",
            "         EVE     0.8843    0.9817    0.9304       218\n",
            "         FAC     0.8552    1.0000    0.9219       124\n",
            "         LOC     0.8615    0.9224    0.8909      1855\n",
            "         MON     0.7778    0.8235    0.8000        51\n",
            "         ORG     0.8709    0.9164    0.8931      2010\n",
            "         PCT     0.7576    0.9259    0.8333        27\n",
            "         PER     0.8749    0.9429    0.9076      1558\n",
            "         PRO     0.8328    0.9929    0.9058       281\n",
            "         TIM     0.5357    0.5556    0.5455        27\n",
            "\n",
            "   micro avg     0.8530    0.9250    0.8875      6330\n",
            "   macro avg     0.7793    0.8810    0.8258      6330\n",
            "weighted avg     0.8557    0.9250    0.8885      6330\n",
            "\n",
            "\n",
            "Aggregate Scores:\n",
            "  Micro F1:    0.8875\n",
            "  Macro F1:    0.8258\n",
            "  Weighted F1: 0.8885\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect prediction errors\n",
        "import random\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ANALYZING PREDICTION ERRORS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "model.eval()\n",
        "error_examples = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for words, chars, labels, masks in test_loader:\n",
        "        words, chars, labels = words.to(device), chars.to(device), labels.to(device)\n",
        "        logits = model(words, chars)\n",
        "        pred = logits.argmax(-1)\n",
        "\n",
        "        # Convert to labels\n",
        "        for i, (pred_seq, true_seq, word_seq) in enumerate(zip(pred, labels, words)):\n",
        "            pred_seq = pred_seq.cpu().tolist()\n",
        "            true_seq = true_seq.cpu().tolist()\n",
        "            word_seq = word_seq.cpu().tolist()\n",
        "\n",
        "            # Get actual tokens (reverse lookup in word2id)\n",
        "            id2word = {v: k for k, v in word2id.items()}\n",
        "            tokens = [id2word.get(w, '<UNK>') for w in word_seq]\n",
        "\n",
        "            pred_tags = []\n",
        "            true_tags = []\n",
        "            token_list = []\n",
        "            has_error = False\n",
        "\n",
        "            for p, t, tok in zip(pred_seq, true_seq, tokens):\n",
        "                if t == -100:  # Skip padding\n",
        "                    continue\n",
        "                pred_tag = id2label[p]\n",
        "                true_tag = id2label[t]\n",
        "\n",
        "                if pred_tag != true_tag:\n",
        "                    has_error = True\n",
        "\n",
        "                pred_tags.append(pred_tag)\n",
        "                true_tags.append(true_tag)\n",
        "                token_list.append(tok)\n",
        "\n",
        "            if has_error and len(token_list) > 0:\n",
        "                error_examples.append({\n",
        "                    'tokens': token_list,\n",
        "                    'predicted': pred_tags,\n",
        "                    'true': true_tags\n",
        "                })\n",
        "\n",
        "print(f\"\\nFound {len(error_examples)} sequences with errors\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RANDOM ERROR EXAMPLES (10 samples)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Show 10 random error examples\n",
        "for idx, example in enumerate(random.sample(error_examples, min(10, len(error_examples))), 1):\n",
        "    print(f\"\\n--- Example {idx} ---\")\n",
        "    print(f\"Tokens:    {' '.join(example['tokens'][:20])}\")  # Show first 20 tokens\n",
        "    print(f\"Predicted: {' '.join(example['predicted'][:20])}\")\n",
        "    print(f\"True:      {' '.join(example['true'][:20])}\")\n",
        "\n",
        "    # Highlight differences\n",
        "    errors = []\n",
        "    for i, (tok, pred, true) in enumerate(zip(example['tokens'][:20], example['predicted'][:20], example['true'][:20])):\n",
        "        if pred != true:\n",
        "            errors.append(f\"  Position {i}: '{tok}' → Predicted: {pred}, True: {true}\")\n",
        "\n",
        "    if errors:\n",
        "        print(\"Errors:\")\n",
        "        for err in errors[:5]:  # Show max 5 errors per example\n",
        "            print(err)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ERROR PATTERN ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Analyze common error patterns\n",
        "all_errors = []\n",
        "for ex in error_examples:\n",
        "    for tok, pred, true in zip(ex['tokens'], ex['predicted'], ex['true']):\n",
        "        if pred != true:\n",
        "            all_errors.append((true, pred))\n",
        "\n",
        "error_counts = Counter(all_errors)\n",
        "print(\"\\nMost common errors (True → Predicted):\")\n",
        "for (true_label, pred_label), count in error_counts.most_common(20):\n",
        "    print(f\"  {true_label:15s} → {pred_label:15s} : {count:4d} times\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejx91hattUTT",
        "outputId": "2efa44d4-95ae-46b6-d93c-7aae23da3ac0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ANALYZING PREDICTION ERRORS\n",
            "================================================================================\n",
            "\n",
            "Found 568 sequences with errors\n",
            "\n",
            "================================================================================\n",
            "RANDOM ERROR EXAMPLES (10 samples)\n",
            "================================================================================\n",
            "\n",
            "--- Example 1 ---\n",
            "Tokens:    تازه یا خشک کرده » ، « زعفران در بسته\\u200cبندی بیش از 30 گرم » و « انجیر ، تازه\n",
            "Predicted: O O O O O O O O O O O O O O O O O B-PRO O O\n",
            "True:      O O O O O O O O O O O O O O O O O O O O\n",
            "Errors:\n",
            "  Position 17: 'انجیر' → Predicted: B-PRO, True: O\n",
            "\n",
            "--- Example 2 ---\n",
            "Tokens:    برنامه نیستان رادیو فرهنگ ، برنامه\\u200cای با موضوع موسیقی ایرانی است که هر روز ساعت 14:30 پخش می\\u200cشود .\n",
            "Predicted: O O O I-PER O O O O O O O O O O B-TIM I-TIM O O O\n",
            "True:      O O B-ORG I-ORG O O O O O O O O O O B-TIM I-TIM O O O\n",
            "Errors:\n",
            "  Position 2: 'رادیو' → Predicted: O, True: B-ORG\n",
            "  Position 3: 'فرهنگ' → Predicted: I-PER, True: I-ORG\n",
            "\n",
            "--- Example 3 ---\n",
            "Tokens:    این استودیو شامل چند واحد تولید ، یک شهربازی و مرکز آموزش خواهد بود .\n",
            "Predicted: O O O O O O O O O O B-ORG I-ORG O O O\n",
            "True:      O O O O O O O O O O O O O O O\n",
            "Errors:\n",
            "  Position 10: 'مرکز' → Predicted: B-ORG, True: O\n",
            "  Position 11: 'آموزش' → Predicted: I-ORG, True: O\n",
            "\n",
            "--- Example 4 ---\n",
            "Tokens:    به گزارش انتخاب ، اقلام عمده صادراتی ایران به ژاپن در چهار ماهه آغازین سال جاری عبارتند\n",
            "Predicted: O O B-ORG O O O O B-LOC O B-LOC O B-DAT I-DAT O B-DAT I-DAT O\n",
            "True:      O O B-ORG O O O O B-LOC O B-LOC O B-DAT I-DAT I-DAT I-DAT I-DAT O\n",
            "Errors:\n",
            "  Position 13: 'آغازین' → Predicted: O, True: I-DAT\n",
            "  Position 14: 'سال' → Predicted: B-DAT, True: I-DAT\n",
            "\n",
            "--- Example 5 ---\n",
            "Tokens:    به گزارش ایسنا ، به نقل از خبرگزاری فرانسه ، شبکه خبری NHK ژاپن به همراه پایگاه خبری \" جی\n",
            "Predicted: O O B-ORG O O O O B-ORG I-ORG O B-ORG I-ORG I-ORG I-ORG O O B-ORG I-ORG I-ORG I-ORG\n",
            "True:      O O B-ORG O O O O B-ORG I-ORG O B-ORG I-ORG I-ORG I-ORG O O B-ORG I-ORG I-ORG I-ORG\n",
            "\n",
            "--- Example 6 ---\n",
            "Tokens:    لیلا رحبی در واکنش به حرف\\u200cهای احسان مهاجر شجاعی نسبت به مسابقه او در المپیک گفت : شنیدم که احسان\n",
            "Predicted: B-PER I-PER O O O O B-PER I-PER I-PER O O O O O O O O O O B-PER\n",
            "True:      B-PER I-PER O O O O B-PER I-PER I-PER O O O O O O O O O O B-PER\n",
            "\n",
            "--- Example 7 ---\n",
            "Tokens:    رویکرد اولیه کارهای چاووشی در مضمون و اجرا ناظر به طیفی از جوانان و نوجوانان بود که شاید بتوان از\n",
            "Predicted: O O O O O O O O O O O O O O O O O O O O\n",
            "True:      O O O B-PER O O O O O O O O O O O O O O O O\n",
            "Errors:\n",
            "  Position 3: 'چاووشی' → Predicted: O, True: B-PER\n",
            "\n",
            "--- Example 8 ---\n",
            "Tokens:    وی در ادامه عنوان کرد : داوری سرپرست جدید فدراسیون ناشنوایان 40 روز که این فدراسیون را تحویل گرفته است\n",
            "Predicted: O O O O O O O O O B-ORG I-ORG B-DAT I-TIM O O O O O O O\n",
            "True:      O O O O O O B-PER O O B-ORG I-ORG O O O O O O O O O\n",
            "Errors:\n",
            "  Position 6: 'داوری' → Predicted: O, True: B-PER\n",
            "  Position 11: '40' → Predicted: B-DAT, True: O\n",
            "  Position 12: 'روز' → Predicted: I-TIM, True: O\n",
            "\n",
            "--- Example 9 ---\n",
            "Tokens:    خبرنگار : حسین مجرد * * انتشاردهنده : امیرحسین کبیری 7285/2007\n",
            "Predicted: O O B-PER I-PER O O O O B-PER I-PER B-DAT\n",
            "True:      O O B-PER I-PER O O O O B-PER I-PER O\n",
            "Errors:\n",
            "  Position 10: '7285/2007' → Predicted: B-DAT, True: O\n",
            "\n",
            "--- Example 10 ---\n",
            "Tokens:    به گزارش فارس ؛ تعداد خریداران خارجی در تالار سبز ایران ، در حالی به 51 خریدار رسیده که از\n",
            "Predicted: O O B-ORG O O O O O B-LOC I-LOC I-LOC O O O O O O O O O\n",
            "True:      O O B-ORG O O O O O O O B-LOC O O O O O O O O O\n",
            "Errors:\n",
            "  Position 8: 'تالار' → Predicted: B-LOC, True: O\n",
            "  Position 9: 'سبز' → Predicted: I-LOC, True: O\n",
            "  Position 10: 'ایران' → Predicted: I-LOC, True: B-LOC\n",
            "\n",
            "================================================================================\n",
            "ERROR PATTERN ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Most common errors (True → Predicted):\n",
            "  O               → B-LOC           :  177 times\n",
            "  O               → B-PER           :  154 times\n",
            "  O               → I-LOC           :  147 times\n",
            "  O               → I-ORG           :  133 times\n",
            "  I-ORG           → O               :  133 times\n",
            "  O               → B-ORG           :  125 times\n",
            "  O               → B-DAT           :   82 times\n",
            "  B-ORG           → O               :   70 times\n",
            "  O               → I-EVE           :   69 times\n",
            "  O               → I-PER           :   67 times\n",
            "  O               → I-DAT           :   60 times\n",
            "  I-DAT           → O               :   36 times\n",
            "  O               → B-PRO           :   33 times\n",
            "  O               → I-PRO           :   32 times\n",
            "  B-LOC           → B-ORG           :   31 times\n",
            "  B-PER           → O               :   31 times\n",
            "  I-ORG           → B-ORG           :   28 times\n",
            "  B-LOC           → I-LOC           :   26 times\n",
            "  B-LOC           → O               :   22 times\n",
            "  B-DAT           → O               :   22 times\n"
          ]
        }
      ]
    }
  ]
}