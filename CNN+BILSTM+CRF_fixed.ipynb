{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-nH7AqQVvsb",
        "outputId": "1f234dd4-75d6-4bb7-ec80-7a25cb1a8511"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from seqeval) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.12/dist-packages (from seqeval) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=ceb77ff71ee4f8aacfd3be44333522c8a0dc9848fcf7fec8d4a33c623540d987\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/b8/73/0b2c1a76b701a677653dd79ece07cfabd7457989dbfbdcd8d7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jpKMPcetUg2m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from seqeval.metrics import classification_report, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "set_seed()"
      ],
      "metadata": {
        "id": "RCngU7aZWA71"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"AliFartout/PEYMA-ARMAN-Mixed\")\n",
        "train_data = dataset[\"train\"]\n",
        "val_data   = dataset[\"validation\"]\n",
        "test_data  = dataset[\"test\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449,
          "referenced_widgets": [
            "3fa2cd0243c941dcb19983f999932b3e",
            "6c5480a4458249cab815cd6737b1e534",
            "89a9eed2341a4c5a8f8559d9db96788c",
            "9b7147f577df4eb289254dba890d479a",
            "6db17a0f15d24af1ac62e7ac6e18b6a9",
            "a1c32e5035844da79135c57b3628eeaf",
            "c390f6bdc21842dbaf2abfa1ccae280e",
            "c2acb1e1888543bb9836a756d5efe4f2",
            "e3af24473f8446a39369ab2efef28714",
            "483e8aa86d4847cb87dfa1055d6507c0",
            "460ed8c9299d49ca815171ca0236d74d",
            "77104c246d53489a87ef2a28e93b1e6d",
            "715c66e4c69d4115b8617eb6cb5e3eb2",
            "48085547066b43469d00769a32a480af",
            "b7c217500d644fb7bda41588b7f00060",
            "72a0dc1e01f248c1a5997d0aee61e40c",
            "c517e7f2731949339bbfd93df6647111",
            "7d41dd6f2dc949eebf2de71fcc487578",
            "ae3df14e300a479ca592b7d58322592f",
            "3ea3419b35f04a61a5abccdef4f5832f",
            "4ae2f2e9f0a447d99fd87ef3ce6b6f5f",
            "b919458f431d4b5f8b7d229e44e7dbaf",
            "4c9b4bb333c74d00841b83d5ddacc925",
            "8a3d6b4b4d804d7c91a3757c504f2ac2",
            "db4f352496004e2d8fcd2f3338b4c252",
            "5cae0a849c4a44648621280a36c500f7",
            "cedf8c63363d4af8a4554149ad536064",
            "ea51dee3960c41ed84e065cda539c11f",
            "05cf625598a841a899ffa53602590d59",
            "e6e8c5fff7f943568c3d440e59f72d13",
            "de1f01f6b3ae45489eabbda41b705ca3",
            "1a466ef65be84fdc9583c85f0a40a8ca",
            "9072748ef7e4425f9ba8d4f657ae837c",
            "eba16c92fb49429ebd57ffdb0e639337",
            "4d8234579bc542c2995b0b225a757a2d",
            "149e37a76ea14d4495974606c7f82d57",
            "8df819b0fc904fa39acf4cf90f8afb16",
            "ffbf4a2d9c1c445aa1675469a898d3a4",
            "96d16d01849d41fb8a852a78e700310e",
            "50d4bf67b6e145fb8da55ce8a225bac2",
            "af5ce038680844a8af523f8229cf122e",
            "18387d3e9c1642239d46472fb3fdebbf",
            "4ac441b552e04bc998b15daa55462494",
            "3f3426b3b44a4e2f8cfaab78d8eeef5c",
            "0f7e3363dece4f68907728ab01843933",
            "9300dc3637ca44ae83d18b71d70c6650",
            "bf409bdbb546475c9b00abb7d17c9404",
            "ebe3f4fedcc040c6b4957b3aa7167910",
            "3885384d3293431c95fa77b5ef9e3759",
            "dbda2bce62254a1e9a47369c2d96ad37",
            "0ad4b268f329440ca43fe85d4f94e948",
            "851a2a9051d2492a9f0b1a545b041cb9",
            "a443e7877b574fc2bd4fbcd65dea274d",
            "43f0f4966b8e4a3fa64187a18f8c42e3",
            "62214b18d5d84a3c9c180b7979071177",
            "308a8183054340cf86a894292454acc8",
            "3661727a5b764828ab36271b09d7c012",
            "8fc0f8c9ee3c4398b6a08d5113b7eab6",
            "d207e4d033f54c749735c783e474dd86",
            "dc54349a02c64692953c4ca5bb57db71",
            "dedbfd26d3d14538a6fa4e54bd4c5339",
            "c33ff1b1b8a44dfd9b648f74ddbc8e88",
            "15e74276ded9459681f8f34363b259fb",
            "f8772de60e684b09873617e60a41fba4",
            "9d3c91261dc445019f874331e79f73cf",
            "a3d11a8d8b9148e6acb708df3ba56d79",
            "5fdab6e56edf4c9591a035299c45bfc5",
            "e69a32619974464f956e2113854a2a8f",
            "30086d25f750458d93ed2fd7b92c5905",
            "522d226851d849f98fff8214722724a2",
            "c3a24b9da3c34b57a5c617f87d5425da",
            "726e9faba7804ab3b14a4847c262be6e",
            "6136c75bee4748c6a6a4764c15c37ecb",
            "c281705e44a64f68a44bd4f57cc628ce",
            "eb281a10e12b4ef39040d931d72874d2",
            "f8375b8fbef146e09507ea14ba87050d",
            "32321134e75a4221bcd13264c494cead"
          ]
        },
        "id": "3374z0IQWDN1",
        "outputId": "33f40850-2728-4f5a-b587-26db3331ec74"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00000-of-00001.parquet:   0%|          | 0.00/3.31M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/validation-00000-of-00001.parquet:   0%|          | 0.00/431k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/test-00000-of-00001.parquet:   0%|          | 0.00/423k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/26384 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3296 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/3296 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_tags(tags):\n",
        "    \"\"\"Convert tags to IOB2 format (B-ORG, I-ORG) for seqeval compatibility\"\"\"\n",
        "    return [t.replace(\"_\", \"-\") for t in tags]\n",
        "\n",
        "# CRITICAL: Normalize and collect labels at the same time\n",
        "all_words, all_chars, all_labels = set(), set(), set()\n",
        "normalized_data = {\"train\": [], \"validation\": [], \"test\": []}\n",
        "\n",
        "for split_name, split in [(\"train\", train_data), (\"validation\", val_data), (\"test\", test_data)]:\n",
        "    for sample in split:\n",
        "        tokens = sample[\"tokens\"]\n",
        "        normalized_tags = normalize_tags(sample[\"ner_tags_names\"])\n",
        "\n",
        "        # Store normalized version\n",
        "        normalized_data[split_name].append({\n",
        "            \"tokens\": tokens,\n",
        "            \"ner_tags_names\": normalized_tags\n",
        "        })\n",
        "\n",
        "        # Collect vocabulary\n",
        "        for w, t in zip(tokens, normalized_tags):\n",
        "            all_words.add(w)\n",
        "            all_labels.add(t)\n",
        "            all_chars.update(list(w))\n",
        "\n",
        "# Replace original data with normalized data\n",
        "train_data = normalized_data[\"train\"]\n",
        "val_data = normalized_data[\"validation\"]\n",
        "test_data = normalized_data[\"test\"]"
      ],
      "metadata": {
        "id": "3fIwSaEtWJgu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2id = {\"<PAD>\":0, \"<UNK>\":1}\n",
        "for w in all_words:\n",
        "    word2id[w] = len(word2id)\n",
        "\n",
        "char2id = {\"<PAD>\":0, \"<UNK>\":1}\n",
        "for ch in all_chars:\n",
        "    char2id[ch] = len(char2id)\n",
        "\n",
        "# Build label encoder AFTER normalization\n",
        "all_labels.add(\"O\")  # Ensure O label exists\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(sorted(list(all_labels)))\n",
        "\n",
        "# Create mapping for quick inverse transform\n",
        "id2label = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
        "print(f\"Sample labels: {list(id2label.values())[:10]}\")\n",
        "print(f\"Total labels: {len(id2label)}\")\n",
        "print(f\"Has O label: {'O' in id2label.values()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIA1gibeWO8j",
        "outputId": "3ecac5fa-3093-42db-e4a2-a0fdcc6720bb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample labels: [np.str_('B-DAT'), np.str_('B-EVE'), np.str_('B-FAC'), np.str_('B-LOC'), np.str_('B-MON'), np.str_('B-ORG'), np.str_('B-PCT'), np.str_('B-PER'), np.str_('B-PRO'), np.str_('B-TIM')]\n",
            "Total labels: 21\n",
            "Has O label: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.vec.gz\n",
        "!gunzip cc.fa.300.vec.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jog_6hZCWtsq",
        "outputId": "1c23f460-bac6-4e02-c23a-7e47c3c1e6d6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-23 10:55:38--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.108, 3.163.189.14, 3.163.189.51, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1258183862 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.fa.300.vec.gz’\n",
            "\n",
            "cc.fa.300.vec.gz    100%[===================>]   1.17G  82.6MB/s    in 16s     \n",
            "\n",
            "2025-11-23 10:55:55 (72.9 MB/s) - ‘cc.fa.300.vec.gz’ saved [1258183862/1258183862]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Load FastText embeddings\n",
        "# -------------------------------\n",
        "EMB_DIM = 300\n",
        "fasttext_path = \"/content/cc.fa.300.vec\"\n",
        "\n",
        "print(\"Loading FastText vectors...\")\n",
        "fasttext = {}\n",
        "with open(fasttext_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "    next(f)  # skip header\n",
        "    for line in tqdm(f, total=2000000):\n",
        "        parts = line.rstrip().split(\" \")\n",
        "        if len(parts) < EMB_DIM + 1:\n",
        "            continue\n",
        "        fasttext[parts[0]] = np.asarray(parts[1:], dtype=\"float32\")\n",
        "\n",
        "embedding_matrix = np.zeros((len(word2id), EMB_DIM), dtype=\"float32\")\n",
        "oov_count = 0\n",
        "for word, idx in word2id.items():\n",
        "    if word in fasttext:\n",
        "        embedding_matrix[idx] = fasttext[word]\n",
        "    else:\n",
        "        embedding_matrix[idx] = np.random.normal(scale=0.1, size=(EMB_DIM,))\n",
        "        oov_count += 1\n",
        "print(f\"OOV words: {oov_count} / {len(word2id)}\")\n",
        "embedding_matrix = torch.tensor(embedding_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j54enopLWU-2",
        "outputId": "d35f0d2c-d679-45f1-c0ec-541a8f9d43b9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading FastText vectors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000000/2000000 [02:36<00:00, 12775.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOV words: 7493 / 29408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NERDataset(Dataset):\n",
        "    def __init__(self, split):\n",
        "        self.data = split\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self.data[idx][\"tokens\"]\n",
        "        labels = self.data[idx][\"ner_tags_names\"]\n",
        "\n",
        "        word_ids = torch.tensor([word2id.get(w, 1) for w in tokens])\n",
        "        char_ids = [torch.tensor([char2id.get(c, 1) for c in w]) for w in tokens]\n",
        "        label_ids = torch.tensor(label_encoder.transform(labels))\n",
        "\n",
        "        return word_ids, char_ids, label_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "def pad_batch(batch):\n",
        "    word_seqs, char_seqs, label_seqs = zip(*batch)\n",
        "    max_len = max(len(w) for w in word_seqs)\n",
        "\n",
        "    padded_words, padded_labels, padded_chars, masks = [], [], [], []\n",
        "\n",
        "    # Use -100 as padding label\n",
        "    PAD_LABEL = -100\n",
        "\n",
        "    for w, cseq, l in zip(word_seqs, char_seqs, label_seqs):\n",
        "        pad_len = max_len - len(w)\n",
        "        padded_words.append(torch.cat([w, torch.zeros(pad_len, dtype=torch.long)]))\n",
        "        padded_labels.append(torch.cat([l, torch.full((pad_len,), PAD_LABEL, dtype=torch.long)]))\n",
        "\n",
        "        # Create mask: 1 for real tokens, 0 for padding\n",
        "        mask = torch.cat([torch.ones(len(w)), torch.zeros(pad_len)])\n",
        "        masks.append(mask)\n",
        "\n",
        "        cseq = cseq + [torch.zeros(1, dtype=torch.long)] * pad_len\n",
        "        padded_chars.append(cseq)\n",
        "\n",
        "    max_char_len = max(len(c) for seq in padded_chars for c in seq)\n",
        "    final_chars = []\n",
        "    for seq in padded_chars:\n",
        "        padded = []\n",
        "        for c in seq:\n",
        "            pad_len = max_char_len - len(c)\n",
        "            padded.append(torch.cat([c, torch.zeros(pad_len, dtype=torch.long)]))\n",
        "        final_chars.append(torch.stack(padded))\n",
        "\n",
        "    return (torch.stack(padded_words),\n",
        "            torch.stack(final_chars),\n",
        "            torch.stack(padded_labels),\n",
        "            torch.stack(masks))\n",
        "\n",
        "train_loader = DataLoader(NERDataset(train_data), batch_size=16, shuffle=True, collate_fn=pad_batch)\n",
        "val_loader   = DataLoader(NERDataset(val_data), batch_size=16, collate_fn=pad_batch)\n",
        "test_loader  = DataLoader(NERDataset(test_data), batch_size=16, collate_fn=pad_batch)\n"
      ],
      "metadata": {
        "id": "gFBvTsNwXx5O"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CRF(nn.Module):\n",
        "    \"\"\"Conditional Random Field layer for sequence tagging\"\"\"\n",
        "\n",
        "    def __init__(self, num_tags, batch_first=True):\n",
        "        super(CRF, self).__init__()\n",
        "        self.num_tags = num_tags\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "        # Transition parameters: transitions[i, j] = score of transitioning from tag j to tag i\n",
        "        self.transitions = nn.Parameter(torch.randn(num_tags, num_tags))\n",
        "\n",
        "        # Start and end transitions\n",
        "        self.start_transitions = nn.Parameter(torch.randn(num_tags))\n",
        "        self.end_transitions = nn.Parameter(torch.randn(num_tags))\n",
        "\n",
        "    def forward(self, emissions, mask):\n",
        "        \"\"\"\n",
        "        Compute the log likelihood of the given sequence of tags.\n",
        "        Args:\n",
        "            emissions: (batch_size, seq_length, num_tags)\n",
        "            mask: (batch_size, seq_length)\n",
        "        Returns:\n",
        "            log_likelihood\n",
        "        \"\"\"\n",
        "        if self.batch_first:\n",
        "            emissions = emissions.transpose(0, 1)  # (seq_length, batch_size, num_tags)\n",
        "            mask = mask.transpose(0, 1)  # (seq_length, batch_size)\n",
        "\n",
        "        return emissions, mask\n",
        "\n",
        "    def decode(self, emissions, mask):\n",
        "        \"\"\"\n",
        "        Find the most likely tag sequence using Viterbi algorithm.\n",
        "        Args:\n",
        "            emissions: (batch_size, seq_length, num_tags)\n",
        "            mask: (batch_size, seq_length)\n",
        "        Returns:\n",
        "            best_tags: (batch_size, seq_length)\n",
        "        \"\"\"\n",
        "        if self.batch_first:\n",
        "            emissions = emissions.transpose(0, 1)  # (seq_length, batch_size, num_tags)\n",
        "            mask = mask.transpose(0, 1)  # (seq_length, batch_size)\n",
        "\n",
        "        return self._viterbi_decode(emissions, mask)\n",
        "\n",
        "    def _viterbi_decode(self, emissions, mask):\n",
        "        \"\"\"Viterbi algorithm for finding best path\"\"\"\n",
        "        seq_length, batch_size, num_tags = emissions.shape\n",
        "\n",
        "        # Initialize scores with start transitions\n",
        "        score = self.start_transitions.unsqueeze(0) + emissions[0]\n",
        "        history = []\n",
        "\n",
        "        # Forward pass\n",
        "        for i in range(1, seq_length):\n",
        "            # Broadcast and add transitions\n",
        "            broadcast_score = score.unsqueeze(2)  # (batch, num_tags, 1)\n",
        "            broadcast_emissions = emissions[i].unsqueeze(1)  # (batch, 1, num_tags)\n",
        "\n",
        "            # Compute scores for all possible transitions\n",
        "            next_score = broadcast_score + self.transitions.unsqueeze(0) + broadcast_emissions\n",
        "\n",
        "            # Find best previous tag for each current tag\n",
        "            next_score, indices = next_score.max(dim=1)\n",
        "\n",
        "            # Apply mask\n",
        "            score = torch.where(mask[i].unsqueeze(1).bool(), next_score, score)\n",
        "            history.append(indices)\n",
        "\n",
        "        # Add end transitions\n",
        "        score = score + self.end_transitions.unsqueeze(0)\n",
        "\n",
        "        # Backtrack to find best path\n",
        "        best_tags_list = []\n",
        "        _, best_last_tag = score.max(dim=1)\n",
        "        best_tags = [best_last_tag]\n",
        "\n",
        "        for hist in reversed(history):\n",
        "            best_last_tag = hist.gather(1, best_last_tag.unsqueeze(1)).squeeze(1)\n",
        "            best_tags.append(best_last_tag)\n",
        "\n",
        "        # Reverse to get forward direction\n",
        "        best_tags.reverse()\n",
        "        best_tags = torch.stack(best_tags, dim=0).transpose(0, 1)  # (batch, seq_length)\n",
        "\n",
        "        return best_tags\n",
        "\n",
        "    def neg_log_likelihood(self, emissions, tags, mask):\n",
        "        \"\"\"\n",
        "        Compute negative log likelihood loss.\n",
        "        Args:\n",
        "            emissions: (batch_size, seq_length, num_tags)\n",
        "            tags: (batch_size, seq_length)\n",
        "            mask: (batch_size, seq_length)\n",
        "        \"\"\"\n",
        "        if self.batch_first:\n",
        "            emissions = emissions.transpose(0, 1)  # (seq_length, batch_size, num_tags)\n",
        "            tags = tags.transpose(0, 1)\n",
        "            mask = mask.transpose(0, 1)\n",
        "\n",
        "        # Compute score of gold sequence\n",
        "        gold_score = self._compute_gold_score(emissions, tags, mask)\n",
        "\n",
        "        # Compute partition function (sum of all possible sequences)\n",
        "        forward_score = self._compute_forward_score(emissions, mask)\n",
        "\n",
        "        # NLL = - (gold_score - log(partition))\n",
        "        return (forward_score - gold_score).mean()\n",
        "\n",
        "    def _compute_gold_score(self, emissions, tags, mask):\n",
        "        \"\"\"Compute score of the gold tag sequence\"\"\"\n",
        "        seq_length, batch_size = tags.shape\n",
        "\n",
        "        # Start transition\n",
        "        score = self.start_transitions[tags[0]]\n",
        "\n",
        "        # Emission scores\n",
        "        score += emissions[0].gather(1, tags[0].unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        # Transition scores\n",
        "        for i in range(1, seq_length):\n",
        "            # Transition from tags[i-1] to tags[i]\n",
        "            trans_score = self.transitions[tags[i], tags[i-1]]\n",
        "            emit_score = emissions[i].gather(1, tags[i].unsqueeze(1)).squeeze(1)\n",
        "\n",
        "            # Apply mask\n",
        "            score = score + torch.where(mask[i].bool(), trans_score + emit_score, torch.zeros_like(trans_score))\n",
        "\n",
        "        # End transition (use last valid tag per sequence)\n",
        "        last_tag_indices = mask.long().sum(0) - 1\n",
        "        last_tags = tags.gather(0, last_tag_indices.unsqueeze(0)).squeeze(0)\n",
        "        score = score + self.end_transitions[last_tags]\n",
        "\n",
        "        return score\n",
        "\n",
        "    def _compute_forward_score(self, emissions, mask):\n",
        "        \"\"\"Compute partition function using forward algorithm\"\"\"\n",
        "        seq_length, batch_size, num_tags = emissions.shape\n",
        "\n",
        "        # Initialize with start transitions\n",
        "        score = self.start_transitions.unsqueeze(0) + emissions[0]\n",
        "\n",
        "        # Forward pass\n",
        "        for i in range(1, seq_length):\n",
        "            broadcast_score = score.unsqueeze(2)  # (batch, num_tags, 1)\n",
        "            broadcast_emissions = emissions[i].unsqueeze(1)  # (batch, 1, num_tags)\n",
        "\n",
        "            # Log-sum-exp trick for numerical stability\n",
        "            next_score = broadcast_score + self.transitions.unsqueeze(0) + broadcast_emissions\n",
        "            next_score = torch.logsumexp(next_score, dim=1)\n",
        "\n",
        "            # Apply mask\n",
        "            score = torch.where(mask[i].unsqueeze(1).bool(), next_score, score)\n",
        "\n",
        "        # Add end transitions\n",
        "        score = score + self.end_transitions.unsqueeze(0)\n",
        "\n",
        "        # Log-sum-exp over all final tags\n",
        "        return torch.logsumexp(score, dim=1)"
      ],
      "metadata": {
        "id": "ODgII3BMX4BL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_BiLSTM_CRF(nn.Module):\n",
        "    def __init__(self, embedding_matrix, char_vocab, num_labels, dropout=0.5):\n",
        "        super().__init__()\n",
        "        vocab_size, emb_dim = embedding_matrix.shape\n",
        "\n",
        "        # Word embeddings (fine-tunable)\n",
        "        self.word_emb = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "        self.word_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Character-level CNN\n",
        "        self.char_emb = nn.Embedding(char_vocab, 30, padding_idx=0)\n",
        "        self.char_cnn = nn.Conv1d(30, 100, kernel_size=3, padding=1)\n",
        "        self.char_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # 2-layer BiLSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=emb_dim + 100,\n",
        "            hidden_size=256,\n",
        "            num_layers=2,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Emission layer (LSTM output to tag scores)\n",
        "        self.hidden2tag = nn.Linear(512, num_labels)\n",
        "\n",
        "        # CRF layer\n",
        "        self.crf = CRF(num_labels, batch_first=True)\n",
        "\n",
        "    def forward(self, words, chars, mask):\n",
        "        \"\"\"\n",
        "        Forward pass - returns emissions for CRF\n",
        "        \"\"\"\n",
        "        # Word embeddings\n",
        "        word_embed = self.word_dropout(self.word_emb(words))\n",
        "\n",
        "        # Character embeddings and CNN\n",
        "        B, L, C = chars.shape\n",
        "        chars = chars.long().view(B*L, C)\n",
        "        char_emb = self.char_emb(chars).transpose(1, 2)\n",
        "        char_feat = torch.max(torch.relu(self.char_cnn(char_emb)), dim=2).values\n",
        "        char_feat = self.char_dropout(char_feat.view(B, L, 100))\n",
        "\n",
        "        # Combine word and character features\n",
        "        combined = torch.cat([word_embed, char_feat], dim=-1)\n",
        "\n",
        "        # BiLSTM\n",
        "        lstm_out, _ = self.lstm(combined)\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "\n",
        "        # Emissions (tag scores)\n",
        "        emissions = self.hidden2tag(lstm_out)\n",
        "\n",
        "        return emissions\n",
        "\n",
        "    def loss(self, words, chars, tags, mask):\n",
        "        \"\"\"\n",
        "        Compute CRF loss\n",
        "        \"\"\"\n",
        "        emissions = self.forward(words, chars, mask)\n",
        "        return self.crf.neg_log_likelihood(emissions, tags, mask)\n",
        "\n",
        "    def decode(self, words, chars, mask):\n",
        "        \"\"\"\n",
        "        Decode best tag sequence using Viterbi\n",
        "        \"\"\"\n",
        "        emissions = self.forward(words, chars, mask)\n",
        "        return self.crf.decode(emissions, mask)\n"
      ],
      "metadata": {
        "id": "NwlvNkM9X9Pi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = CNN_BiLSTM_CRF(embedding_matrix, len(char2id), len(label_encoder.classes_)).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
        "\n",
        "# -------------------------------\n",
        "# 11. Helper function to convert predictions to labels\n",
        "# -------------------------------\n",
        "def convert_to_labels(predictions, labels):\n",
        "    \"\"\"Convert tensor predictions to label lists, filtering padding\"\"\"\n",
        "    pred_labels = []\n",
        "    true_labels = []\n",
        "\n",
        "    for pred_seq, true_seq in zip(predictions, labels):\n",
        "        pred_seq = pred_seq.cpu().tolist()\n",
        "        true_seq = true_seq.cpu().tolist()\n",
        "\n",
        "        # Filter out padding (label -100)\n",
        "        pred_tags = []\n",
        "        true_tags = []\n",
        "        for p, t in zip(pred_seq, true_seq):\n",
        "            if t == -100:  # Skip padding\n",
        "                continue\n",
        "            pred_tags.append(id2label[p])\n",
        "            true_tags.append(id2label[t])\n",
        "\n",
        "        if pred_tags:  # Only add non-empty sequences\n",
        "            pred_labels.append(pred_tags)\n",
        "            true_labels.append(true_tags)\n",
        "\n",
        "    return pred_labels, true_labels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0hnJaRZYO_d",
        "outputId": "c075fb20-1c57-410a-9f52-60dc6550f953"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_f1 = 0\n",
        "patience_counter = 0\n",
        "patience = 5\n",
        "num_epochs = 20\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\\n\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for words, chars, labels, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
        "        words = words.to(device)\n",
        "        chars = chars.to(device)\n",
        "        labels_input = labels.clone()\n",
        "        # Replace -100 with 0 for CRF (CRF doesn't handle -100)\n",
        "        labels_input[labels == -100] = 0\n",
        "        labels_input = labels_input.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = model.loss(words, chars, labels_input, masks)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    y_val_true, y_val_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for words, chars, labels, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
        "            words = words.to(device)\n",
        "            chars = chars.to(device)\n",
        "            labels_input = labels.clone()\n",
        "            labels_input[labels == -100] = 0\n",
        "            labels_input = labels_input.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            loss = model.loss(words, chars, labels_input, masks)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Decode using Viterbi\n",
        "            pred = model.decode(words, chars, masks)\n",
        "\n",
        "            pred_labels, true_labels = convert_to_labels(pred, labels)\n",
        "            y_val_pred.extend(pred_labels)\n",
        "            y_val_true.extend(true_labels)\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_f1 = f1_score(y_val_true, y_val_pred, average=\"micro\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f}\")\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(val_f1)\n",
        "\n",
        "    # Early stopping & checkpoint\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        torch.save(model.state_dict(), \"best_crf_model.pt\")\n",
        "        patience_counter = 0\n",
        "        print(f\"  → New best model saved! (F1: {val_f1:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs!\")\n",
        "            break\n",
        "\n",
        "print(f\"\\nTraining completed. Best validation F1: {best_val_f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnojQYU_YZAw",
        "outputId": "0964daa1-3885-4555-ebc2-093851ec341f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting training...\n",
            "Total parameters: 11,771,260\n",
            "Trainable parameters: 11,771,260\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 [Train]: 100%|██████████| 1649/1649 [03:18<00:00,  8.32it/s]\n",
            "Epoch 1/20 [Val]: 100%|██████████| 206/206 [00:13<00:00, 15.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train Loss: 4.1592 | Val Loss: -2.0704 | Val F1: 0.7171\n",
            "  → New best model saved! (F1: 0.7171)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20 [Train]: 100%|██████████| 1649/1649 [03:12<00:00,  8.57it/s]\n",
            "Epoch 2/20 [Val]: 100%|██████████| 206/206 [00:13<00:00, 15.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 02 | Train Loss: -5.6906 | Val Loss: -8.6015 | Val F1: 0.7641\n",
            "  → New best model saved! (F1: 0.7641)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20 [Train]: 100%|██████████| 1649/1649 [03:13<00:00,  8.52it/s]\n",
            "Epoch 3/20 [Val]: 100%|██████████| 206/206 [00:12<00:00, 15.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 03 | Train Loss: -12.4124 | Val Loss: -14.2462 | Val F1: 0.8090\n",
            "  → New best model saved! (F1: 0.8090)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20 [Train]: 100%|██████████| 1649/1649 [03:15<00:00,  8.43it/s]\n",
            "Epoch 4/20 [Val]: 100%|██████████| 206/206 [00:13<00:00, 15.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 04 | Train Loss: -18.7015 | Val Loss: -19.4535 | Val F1: 0.8309\n",
            "  → New best model saved! (F1: 0.8309)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20 [Train]: 100%|██████████| 1649/1649 [03:12<00:00,  8.57it/s]\n",
            "Epoch 5/20 [Val]: 100%|██████████| 206/206 [00:13<00:00, 15.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 05 | Train Loss: -24.7782 | Val Loss: -24.4402 | Val F1: 0.8399\n",
            "  → New best model saved! (F1: 0.8399)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20 [Train]: 100%|██████████| 1649/1649 [03:13<00:00,  8.52it/s]\n",
            "Epoch 6/20 [Val]: 100%|██████████| 206/206 [00:12<00:00, 16.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 06 | Train Loss: -30.7464 | Val Loss: -29.4695 | Val F1: 0.8553\n",
            "  → New best model saved! (F1: 0.8553)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20 [Train]: 100%|██████████| 1649/1649 [03:10<00:00,  8.66it/s]\n",
            "Epoch 7/20 [Val]: 100%|██████████| 206/206 [00:13<00:00, 15.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 07 | Train Loss: -36.4962 | Val Loss: -34.4118 | Val F1: 0.8656\n",
            "  → New best model saved! (F1: 0.8656)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20 [Train]: 100%|██████████| 1649/1649 [03:10<00:00,  8.67it/s]\n",
            "Epoch 8/20 [Val]: 100%|██████████| 206/206 [00:13<00:00, 15.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 08 | Train Loss: -42.1090 | Val Loss: -38.9710 | Val F1: 0.8694\n",
            "  → New best model saved! (F1: 0.8694)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20 [Train]: 100%|██████████| 1649/1649 [03:11<00:00,  8.63it/s]\n",
            "Epoch 9/20 [Val]: 100%|██████████| 206/206 [00:13<00:00, 15.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 09 | Train Loss: -47.7877 | Val Loss: -43.5208 | Val F1: 0.8680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20 [Train]: 100%|██████████| 1649/1649 [03:10<00:00,  8.66it/s]\n",
            "Epoch 10/20 [Val]: 100%|██████████| 206/206 [00:13<00:00, 15.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Train Loss: -53.3075 | Val Loss: -48.3207 | Val F1: 0.8744\n",
            "  → New best model saved! (F1: 0.8744)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20 [Train]: 100%|██████████| 1649/1649 [03:07<00:00,  8.79it/s]\n",
            "Epoch 11/20 [Val]: 100%|██████████| 206/206 [00:13<00:00, 15.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 | Train Loss: -58.7779 | Val Loss: -52.6760 | Val F1: 0.8725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20 [Train]: 100%|██████████| 1649/1649 [03:09<00:00,  8.69it/s]\n",
            "Epoch 12/20 [Val]: 100%|██████████| 206/206 [00:13<00:00, 15.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 | Train Loss: -64.3169 | Val Loss: -57.7546 | Val F1: 0.8775\n",
            "  → New best model saved! (F1: 0.8775)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20 [Train]: 100%|██████████| 1649/1649 [03:09<00:00,  8.70it/s]\n",
            "Epoch 13/20 [Val]: 100%|██████████| 206/206 [00:13<00:00, 15.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 | Train Loss: -69.7876 | Val Loss: -61.9846 | Val F1: 0.8801\n",
            "  → New best model saved! (F1: 0.8801)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20 [Train]: 100%|██████████| 1649/1649 [03:08<00:00,  8.76it/s]\n",
            "Epoch 14/20 [Val]: 100%|██████████| 206/206 [00:12<00:00, 16.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 | Train Loss: -75.2045 | Val Loss: -66.3234 | Val F1: 0.8823\n",
            "  → New best model saved! (F1: 0.8823)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20 [Train]: 100%|██████████| 1649/1649 [03:08<00:00,  8.74it/s]\n",
            "Epoch 15/20 [Val]: 100%|██████████| 206/206 [00:13<00:00, 15.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 | Train Loss: -80.5831 | Val Loss: -71.3915 | Val F1: 0.8800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20 [Train]: 100%|██████████| 1649/1649 [03:08<00:00,  8.75it/s]\n",
            "Epoch 16/20 [Val]: 100%|██████████| 206/206 [00:13<00:00, 15.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 | Train Loss: -85.8682 | Val Loss: -74.4305 | Val F1: 0.8779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20 [Train]: 100%|██████████| 1649/1649 [03:06<00:00,  8.82it/s]\n",
            "Epoch 17/20 [Val]: 100%|██████████| 206/206 [00:13<00:00, 15.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 | Train Loss: -91.1835 | Val Loss: -79.4352 | Val F1: 0.8801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20 [Train]: 100%|██████████| 1649/1649 [03:08<00:00,  8.75it/s]\n",
            "Epoch 18/20 [Val]: 100%|██████████| 206/206 [00:13<00:00, 15.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 | Train Loss: -95.6122 | Val Loss: -82.2686 | Val F1: 0.8898\n",
            "  → New best model saved! (F1: 0.8898)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20 [Train]: 100%|██████████| 1649/1649 [03:08<00:00,  8.76it/s]\n",
            "Epoch 19/20 [Val]: 100%|██████████| 206/206 [00:12<00:00, 16.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 | Train Loss: -98.5758 | Val Loss: -84.0296 | Val F1: 0.8886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20 [Train]: 100%|██████████| 1649/1649 [03:08<00:00,  8.74it/s]\n",
            "Epoch 20/20 [Val]: 100%|██████████| 206/206 [00:13<00:00, 15.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 | Train Loss: -101.5111 | Val Loss: -86.5059 | Val F1: 0.8891\n",
            "\n",
            "Training completed. Best validation F1: 0.8898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nLoading best model for test evaluation...\")\n",
        "model.load_state_dict(torch.load(\"best_crf_model.pt\"))\n",
        "model.eval()\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for words, chars, labels, masks in tqdm(test_loader, desc=\"Testing\"):\n",
        "        words = words.to(device)\n",
        "        chars = chars.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        # Decode using Viterbi\n",
        "        pred = model.decode(words, chars, masks)\n",
        "\n",
        "        pred_labels, true_labels = convert_to_labels(pred, labels)\n",
        "        y_pred.extend(pred_labels)\n",
        "        y_true.extend(true_labels)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL TEST RESULTS (CNN-BiLSTM-CRF)\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, digits=4))\n",
        "print(\"\\nAggregate Scores:\")\n",
        "print(f\"  Micro F1:    {f1_score(y_true, y_pred, average='micro'):.4f}\")\n",
        "print(f\"  Macro F1:    {f1_score(y_true, y_pred, average='macro'):.4f}\")\n",
        "print(f\"  Weighted F1: {f1_score(y_true, y_pred, average='weighted'):.4f}\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U012pDqDoHdI",
        "outputId": "be6e72a3-a4ef-4dac-ad66-95af59059f87"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading best model for test evaluation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 206/206 [00:08<00:00, 23.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "FINAL TEST RESULTS (CNN-BiLSTM-CRF)\n",
            "================================================================================\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         DAT     0.5792    0.7151    0.6400       179\n",
            "         EVE     0.9559    0.9954    0.9753       218\n",
            "         FAC     0.8732    1.0000    0.9323       124\n",
            "         LOC     0.9137    0.9245    0.9191      1855\n",
            "         MON     0.6935    0.8431    0.7611        51\n",
            "         ORG     0.8699    0.9050    0.8871      2010\n",
            "         PCT     0.8519    0.8519    0.8519        27\n",
            "         PER     0.9342    0.9198    0.9269      1558\n",
            "         PRO     0.8978    1.0000    0.9461       281\n",
            "         TIM     0.5333    0.5926    0.5614        27\n",
            "\n",
            "   micro avg     0.8889    0.9161    0.9023      6330\n",
            "   macro avg     0.8103    0.8747    0.8401      6330\n",
            "weighted avg     0.8917    0.9161    0.9033      6330\n",
            "\n",
            "\n",
            "Aggregate Scores:\n",
            "  Micro F1:    0.9023\n",
            "  Macro F1:    0.8401\n",
            "  Weighted F1: 0.9033\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}