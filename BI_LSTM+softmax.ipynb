{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2c9QRhdbkwH"
      },
      "source": [
        "import stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRX-wR3RnS9U"
      },
      "outputs": [],
      "source": [
        "import pyconll\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, TimeDistributed\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l30R-DUFfpMR",
        "outputId": "6852ced8-d579-48ed-bfc5-efb8aa85a360"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyconll\n",
            "  Downloading pyconll-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Downloading pyconll-3.3.1-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: pyconll\n",
            "Successfully installed pyconll-3.3.1\n"
          ]
        }
      ],
      "source": [
        "#!pip install pyconll\n",
        "#install this cause in every runtime need to install it again ( its to use conllu files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FLRRVYyBbm5r",
        "outputId": "bdbe17f5-7967-48d1-c8f7-f53bb5e820bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Train sentences: 26196, Dev sentences: 1456, Test sentences: 1455\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,201,200</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">234,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,369</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │     \u001b[38;5;34m3,201,200\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m234,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m17\u001b[0m)        │         \u001b[38;5;34m4,369\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,440,065</span> (13.12 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,440,065\u001b[0m (13.12 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,440,065</span> (13.12 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,440,065\u001b[0m (13.12 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "\n",
            "Training Bi-LSTM (Softmax) model for 2 epochs...\n",
            "Epoch 1/2\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m483s\u001b[0m 573ms/step - accuracy: 0.9191 - loss: 0.3197 - val_accuracy: 0.9894 - val_loss: 0.0341\n",
            "Epoch 2/2\n",
            "\u001b[1m819/819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m468s\u001b[0m 571ms/step - accuracy: 0.9916 - loss: 0.0282 - val_accuracy: 0.9914 - val_loss: 0.0265\n",
            "\n",
            "========== Evaluation on Development (Dev) Set ==========\n",
            "Overall Tagging Accuracy: 0.9499461142378158\n",
            "\n",
            "Detailed Metrics (Precision, Recall, F1-Score) per UPOS Tag:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ     0.8628    0.8665    0.8646      1872\n",
            "         ADP     0.9770    0.9913    0.9841      3553\n",
            "         ADV     0.9388    0.8825    0.9098       417\n",
            "         AUX     0.9889    0.9770    0.9829       912\n",
            "       CCONJ     0.9964    0.9982    0.9973      1106\n",
            "         DET     0.9218    0.9701    0.9454       535\n",
            "        INTJ     0.9444    0.8293    0.8831        41\n",
            "        NOUN     0.9399    0.9432    0.9415      8289\n",
            "         NUM     0.9158    0.9283    0.9220       293\n",
            "        PART     1.0000    0.6316    0.7742        19\n",
            "        PRON     0.9866    0.9688    0.9776      1219\n",
            "       PROPN     0.7945    0.8146    0.8045      1106\n",
            "       PUNCT     1.0000    0.9996    0.9998      2318\n",
            "       SCONJ     0.9883    0.9616    0.9748       703\n",
            "        VERB     0.9820    0.9596    0.9706      2670\n",
            "\n",
            "    accuracy                         0.9499     25053\n",
            "   macro avg     0.9492    0.9148    0.9288     25053\n",
            "weighted avg     0.9503    0.9499    0.9500     25053\n",
            "\n",
            "==================================================\n",
            "\n",
            "========== Evaluation on Final Test Set ==========\n",
            "Overall Tagging Accuracy: 0.9515601044213318\n",
            "\n",
            "Detailed Metrics (Precision, Recall, F1-Score) per UPOS Tag:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ     0.8695    0.8632    0.8663      1652\n",
            "         ADP     0.9814    0.9897    0.9855      3407\n",
            "         ADV     0.9243    0.8906    0.9072       384\n",
            "         AUX     0.9898    0.9733    0.9815       899\n",
            "       CCONJ     0.9961    1.0000    0.9981      1026\n",
            "         DET     0.9189    0.9695    0.9435       491\n",
            "        INTJ     0.8696    0.7407    0.8000        27\n",
            "        NOUN     0.9527    0.9394    0.9460      8219\n",
            "         NUM     0.9020    0.9420    0.9215       293\n",
            "        PART     1.0000    0.8571    0.9231        28\n",
            "        PRON     0.9829    0.9680    0.9754      1126\n",
            "       PROPN     0.7480    0.8254    0.7848      1111\n",
            "       PUNCT     1.0000    1.0000    1.0000      2141\n",
            "       SCONJ     0.9791    0.9620    0.9705       632\n",
            "        VERB     0.9811    0.9829    0.9820      2696\n",
            "           X     0.0000    0.0000    0.0000         1\n",
            "\n",
            "    accuracy                         0.9516     24133\n",
            "   macro avg     0.8810    0.8690    0.8741     24133\n",
            "weighted avg     0.9525    0.9516    0.9519     24133\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# --- CONFIGURATION AND PATHS ---\n",
        "TRAIN_PATH = 'PosData/fa_perdt-ud-train.conllu'\n",
        "DEV_PATH = 'PosData/fa_perdt-ud-dev.conllu'\n",
        "TEST_PATH = 'PosData/fa_perdt-ud-test.conllu'\n",
        "\n",
        "# Hyperparameters\n",
        "MAX_LEN = 100         # All sequences will be padded/truncated to this length typical\n",
        "EMBEDDING_DIM = 100   # Size of the word embeddings idk why\n",
        "LSTM_UNITS = 128      # Size of the LSTM hidden state idk why\n",
        "EPOCHS = 2           # I did 5 at first but took too much time\n",
        "\n",
        "# --- STEP 1: DATA LOADING ---\n",
        "def load_conllu(path):\n",
        "    \"\"\"Loads CoNLL-U files and extracts sentences as lists of (words, tags).\"\"\"\n",
        "    data = []\n",
        "    try:\n",
        "        conll = pyconll.load_from_file(path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {path}. Returning empty data.\")\n",
        "        return []\n",
        "\n",
        "    for sentence in conll:\n",
        "        # Extract words and UPOS tags, ensuring both exist for a token\n",
        "        words = [token.form for token in sentence if token.form and token.upos]\n",
        "        tags = [token.upos for token in sentence if token.form and token.upos]\n",
        "\n",
        "        # Skip incomplete sentences\n",
        "        if len(words) == len(tags) and len(words) > 0:\n",
        "            data.append((words, tags))\n",
        "    return data\n",
        "\n",
        "print(\"Loading data...\")\n",
        "train_data = load_conllu(TRAIN_PATH)\n",
        "dev_data = load_conllu(DEV_PATH)\n",
        "test_data = load_conllu(TEST_PATH)\n",
        "print(f\"Train sentences: {len(train_data)}, Dev sentences: {len(dev_data)}, Test sentences: {len(test_data)}\")\n",
        "\n",
        "# --- STEP 2: PREPROCESSING (Tokenization and Padding) ---\n",
        "\n",
        "def preprocess_data(data, max_len=MAX_LEN, word_tokenizer=None, tag_tokenizer=None):\n",
        "    \"\"\"Tokenizes words and tags, pads sequences, and one-hot encodes tags.\"\"\"\n",
        "    X_list = [sent[0] for sent in data]\n",
        "    y_list = [sent[1] for sent in data]\n",
        "\n",
        "    # 1. Initialize Tokenizers (fit only on training data)\n",
        "    if word_tokenizer is None:\n",
        "        word_tokenizer = Tokenizer(oov_token='<UNK>', lower=True)\n",
        "        word_tokenizer.fit_on_texts(X_list)\n",
        "\n",
        "    if tag_tokenizer is None:\n",
        "        tag_tokenizer = Tokenizer(lower=False)\n",
        "        tag_tokenizer.fit_on_texts(y_list)\n",
        "\n",
        "    # 2. Convert to Sequences\n",
        "    # Directly tokenize the list of sentences\n",
        "    X_seq = word_tokenizer.texts_to_sequences(X_list)\n",
        "    y_seq = tag_tokenizer.texts_to_sequences(y_list)\n",
        "\n",
        "    # 3. Padding\n",
        "    X_padded = pad_sequences(X_seq, maxlen=max_len, padding='post')\n",
        "    y_padded = pad_sequences(y_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "    # 4. One-Hot Encoding for Softmax Loss (required for y_train only)\n",
        "    # The padding index (0) is kept as all-zeros.\n",
        "    y_categorical = to_categorical(y_padded, num_classes=len(tag_tokenizer.word_index) + 1)\n",
        "\n",
        "    return X_padded, y_categorical, y_padded, word_tokenizer, tag_tokenizer\n",
        "\n",
        "# Preprocess training data and get tokenizers\n",
        "X_train, y_train_cat, y_train_idx, word_tokenizer, tag_tokenizer = preprocess_data(train_data)\n",
        "\n",
        "# Preprocess dev and test data (y_dev_cat is used for validation, y_dev_idx for final metric calculation)\n",
        "X_dev, y_dev_cat, y_dev_idx, _, _ = preprocess_data(dev_data, word_tokenizer=word_tokenizer, tag_tokenizer=tag_tokenizer)\n",
        "X_test, y_test_cat, y_test_idx, _, _ = preprocess_data(test_data, word_tokenizer=word_tokenizer, tag_tokenizer=tag_tokenizer)\n",
        "\n",
        "\n",
        "# Extract vocabulary size and number of tags\n",
        "WORD_VOCAB_SIZE = len(word_tokenizer.word_index) + 1\n",
        "TAG_VOCAB_SIZE = len(tag_tokenizer.word_index) + 1\n",
        "\n",
        "# --- STEP 3: MODEL DEFINITION ---\n",
        "\n",
        "def create_bilstm_softmax_model(word_vocab_size, tag_vocab_size, max_len, embedding_dim, lstm_units):\n",
        "    \"\"\"Defines and compiles the Bi-LSTM model with a final Softmax layer.\"\"\"\n",
        "\n",
        "    input_layer = Input(shape=(max_len,))\n",
        "\n",
        "    # 1. Word Embedding Layer\n",
        "    embedding_layer = Embedding(\n",
        "        input_dim=word_vocab_size,\n",
        "        output_dim=embedding_dim,\n",
        "        input_length=max_len\n",
        "    )(input_layer)\n",
        "\n",
        "    # 2. Bi-LSTM Layer\n",
        "    bilstm_layer = Bidirectional(\n",
        "        LSTM(lstm_units, return_sequences=True, dropout=0.5, recurrent_dropout=0.5)\n",
        "    )(embedding_layer)\n",
        "\n",
        "    # 3. Time Distributed Dense + Softmax\n",
        "    # This applies a softmax classifier to the output of EVERY time step (word)\n",
        "    output_layer = TimeDistributed(Dense(tag_vocab_size, activation='softmax'))(bilstm_layer)\n",
        "\n",
        "    # Define Model and Compile\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    # Use categorical_crossentropy loss for multi-class classification with one-hot targets\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "# Create and compile the model\n",
        "model = create_bilstm_softmax_model(WORD_VOCAB_SIZE, TAG_VOCAB_SIZE, MAX_LEN, EMBEDDING_DIM, LSTM_UNITS)\n",
        "\n",
        "# --- STEP 4: TRAINING ---\n",
        "\n",
        "print(f\"\\nTraining Bi-LSTM (Softmax) model for {EPOCHS} epochs...\")\n",
        "history = model.fit(\n",
        "    X_train, y_train_cat,\n",
        "    batch_size=32,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(X_dev, y_dev_cat),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --- STEP 5: EVALUATION ---\n",
        "\n",
        "def evaluate_model(model, X_data, y_true_padded_idx, tag_tokenizer, dataset_name='set'):\n",
        "    \"\"\"Predicts tags, unpacks padding, and prints metrics.\"\"\"\n",
        "\n",
        "    # Inverse map for converting predicted indices back to tag strings\n",
        "    idx_to_tag = {v: k for k, v in tag_tokenizer.word_index.items()}\n",
        "\n",
        "    # Prediction: Output is a 3D tensor (sequences, max_len, tag_vocab_size)\n",
        "    y_pred_proba = model.predict(X_data, verbose=0)\n",
        "\n",
        "    # Convert probabilities to indices by picking the max probability tag for each word\n",
        "    y_pred_padded_idx = np.argmax(y_pred_proba, axis=-1)\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    # Unpack the sequences, ignoring padding (index 0)\n",
        "    for true_seq, pred_seq in zip(y_true_padded_idx, y_pred_padded_idx):\n",
        "        for true_idx, pred_idx in zip(true_seq, pred_seq):\n",
        "            if true_idx != 0: # 0 is the padding index, which we ignore\n",
        "                # True tag must be looked up from its index\n",
        "                true_tag = idx_to_tag.get(true_idx, '<PAD>')\n",
        "\n",
        "                # Predicted tag must be looked up from its index\n",
        "                pred_tag = idx_to_tag.get(pred_idx, '<PAD>')\n",
        "\n",
        "                if true_tag != '<PAD>':\n",
        "                    y_true.append(true_tag)\n",
        "                    y_pred.append(pred_tag)\n",
        "\n",
        "    print(f'\\n{\"=\"*10} Evaluation on {dataset_name} Set {\"=\"*10}')\n",
        "\n",
        "    if not y_true:\n",
        "        print(\"No valid data for evaluation.\")\n",
        "        return\n",
        "\n",
        "    # Overall Accuracy\n",
        "    print('Overall Tagging Accuracy:', accuracy_score(y_true, y_pred))\n",
        "\n",
        "    # Detailed Metrics (Precision, Recall, F1-Score)\n",
        "    print('\\nDetailed Metrics (Precision, Recall, F1-Score) per UPOS Tag:')\n",
        "\n",
        "    # Use only tags present in the true labels for the report\n",
        "    target_names = sorted(list(set(y_true)))\n",
        "    print(classification_report(y_true, y_pred, labels=target_names, target_names=target_names, digits=4, zero_division=0))\n",
        "    print('='*50)\n",
        "\n",
        "\n",
        "# ===============================================\n",
        "#              MAIN EXECUTION\n",
        "# ===============================================\n",
        "\n",
        "# 1. Evaluate on Development Set (Tuning)\n",
        "evaluate_model(model, X_dev, y_dev_idx, tag_tokenizer, 'Development (Dev)')\n",
        "\n",
        "# 2. Evaluate on Test Set (Final Score)\n",
        "evaluate_model(model, X_test, y_test_idx, tag_tokenizer, 'Final Test')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
