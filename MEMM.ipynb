{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU5QnntKSqVn"
      },
      "source": [
        "import stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uT3UrgJVStHX",
        "outputId": "b3bd062a-93e8-4753-d253-b57318750984"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyconll\n",
            "  Downloading pyconll-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Downloading pyconll-3.3.1-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: pyconll\n",
            "Successfully installed pyconll-3.3.1\n"
          ]
        }
      ],
      "source": [
        "#!pip install pyconll\n",
        "#have to execute it cause not availbale in colab (just remove the #)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Yi-7hUJPSvvg",
        "outputId": "ae78e760-b459-4cb9-cab5-180e22511caf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Building feature vocabulary...\n",
            "Total unique features extracted: 33139\n",
            "\n",
            "--- SHAPE CONFIRMATION ---\n",
            "X_train_tf shape: (26196, 100, 20) (Note the small feature dimension: 20)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ feature_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ time_distributed                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,060,480</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ time_distributed_1              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ time_distributed_2              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)        â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">561</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               â”‚                        â”‚               â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ feature_input (\u001b[38;5;33mInputLayer\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m20\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ time_distributed                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m)    â”‚     \u001b[38;5;34m1,060,480\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mTimeDistributed\u001b[0m)               â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ time_distributed_1              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m32\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mTimeDistributed\u001b[0m)               â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ time_distributed_2              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m17\u001b[0m)        â”‚           \u001b[38;5;34m561\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mTimeDistributed\u001b[0m)               â”‚                        â”‚               â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,061,041</span> (4.05 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,061,041\u001b[0m (4.05 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,061,041</span> (4.05 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,061,041\u001b[0m (4.05 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "\n",
            "Training Lightweight MEMM Approximation Model for 5 epochs...\n",
            "Epoch 1/5\n",
            "\u001b[1m819/819\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 239ms/step - accuracy: 0.8252 - loss: 0.7354 - val_accuracy: 0.9006 - val_loss: 0.3065\n",
            "Epoch 2/5\n",
            "\u001b[1m819/819\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9243 - loss: 0.2614 - val_accuracy: 0.9597 - val_loss: 0.1499\n",
            "Epoch 3/5\n",
            "\u001b[1m819/819\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 17ms/step - accuracy: 0.9594 - loss: 0.1441 - val_accuracy: 0.9672 - val_loss: 0.1093\n",
            "Epoch 4/5\n",
            "\u001b[1m819/819\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9671 - loss: 0.1084 - val_accuracy: 0.9725 - val_loss: 0.0897\n",
            "Epoch 5/5\n",
            "\u001b[1m819/819\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.9723 - loss: 0.0897 - val_accuracy: 0.9757 - val_loss: 0.0773\n",
            "\n",
            "========== Evaluation on Development (Dev) Set ==========\n",
            "Overall Tagging Accuracy: 0.8589390492156628\n",
            "\n",
            "Detailed Metrics (Precision, Recall, F1-Score) per UPOS Tag:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ     0.7333    0.3937    0.5123      1872\n",
            "         ADP     0.9519    0.9527    0.9523      3553\n",
            "         ADV     0.9060    0.6475    0.7552       417\n",
            "         AUX     0.9766    0.9145    0.9445       912\n",
            "       CCONJ     0.9883    0.9892    0.9887      1106\n",
            "         DET     0.8041    0.8131    0.8086       535\n",
            "        INTJ     0.0000    0.0000    0.0000        41\n",
            "        NOUN     0.7618    0.9571    0.8483      8289\n",
            "         NUM     0.8427    0.8225    0.8325       293\n",
            "        PART     0.0000    0.0000    0.0000        19\n",
            "        PRON     0.9292    0.8934    0.9109      1219\n",
            "       PROPN     0.9044    0.1112    0.1981      1106\n",
            "       PUNCT     0.9965    0.9948    0.9957      2318\n",
            "       SCONJ     0.9782    0.8947    0.9346       703\n",
            "        VERB     0.9109    0.9150    0.9129      2670\n",
            "\n",
            "   micro avg     0.8605    0.8589    0.8597     25053\n",
            "   macro avg     0.7789    0.6866    0.7063     25053\n",
            "weighted avg     0.8650    0.8589    0.8403     25053\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n",
            "--- Sample Incorrect Predictions ---\n",
            "\n",
            "[ERROR SENTENCE 1]\n",
            "  WORD            TRUE TAG        PREDICTED TAG  \n",
            "  ---------------------------------------------\n",
            "âœ“ Ø¨Ù‡              ADP             ADP            \n",
            "âœ“ Ú¯Ø²Ø§Ø±Ø´           NOUN            NOUN           \n",
            "âœ“ Ø®Ø¨Ø±Ù†Ú¯Ø§Ø±         NOUN            NOUN           \n",
            "âŒ Ù…Ù‡Ø±             PROPN           NOUN           \n",
            "âœ“ Ø¯Ø±              ADP             ADP            \n",
            "âŒ Ú¯Ø±Ú¯Ø§Ù†           PROPN           NOUN           \n",
            "âœ“ ØŒ               PUNCT           PUNCT          \n",
            "âœ“ Ø¨Ø±              ADP             ADP            \n",
            "âœ“ Ø§Ø³Ø§Ø³            NOUN            NOUN           \n",
            "âœ“ Ø¨Ø§ÙˆØ±Ù‡Ø§ÛŒ         NOUN            NOUN           \n",
            "âœ“ Ø¯ÛŒÙ†ÛŒ            ADJ             ADJ            \n",
            "âŒ ØªØ±Ú©Ù…Ù†â€ŒÙ‡Ø§        PROPN           NOUN           \n",
            "âœ“ Ø¯Ø±              ADP             ADP            \n",
            "âœ“ Ø§ÛŒÙ†             DET             DET            \n",
            "âœ“ Ø±ÙˆØ²             NOUN            NOUN           \n",
            "âœ“ Ø¨Ø±Ø§ÛŒ            ADP             ADP            \n",
            "âŒ Ù¾ÛŒØ§Ù…Ø¨Ø±          PROPN           NOUN           \n",
            "âœ“ Ø§Ú©Ø±Ù…            PROPN           PROPN          \n",
            "âœ“ (               PUNCT           PUNCT          \n",
            "âœ“ Øµ               NOUN            NOUN           \n",
            "âœ“ )               PUNCT           PUNCT          \n",
            "âœ“ Ù†Ø§Ø±Ø§Ø­ØªÛŒ         NOUN            NOUN           \n",
            "âœ“ Ùˆ               CCONJ           CCONJ          \n",
            "âœ“ Ø¨ÛŒÙ…Ø§Ø±ÛŒ          NOUN            NOUN           \n",
            "âœ“ Ø±Ø®              NOUN            NOUN           \n",
            "âœ“ Ø¯Ø§Ø¯             VERB            VERB           \n",
            "âœ“ Ú©Ù‡              SCONJ           SCONJ          \n",
            "âœ“ Ú†Ù†Ø¯             DET             DET            \n",
            "âœ“ Ø±ÙˆØ²             NOUN            NOUN           \n",
            "âŒ Ø¨Ø¹Ø¯             ADJ             ADP            \n",
            "âœ“ Ø¨Ø§              ADP             ADP            \n",
            "âœ“ Ø±Ø­Ù„Øª            NOUN            NOUN           \n",
            "âŒ Ù†Ø¨ÛŒ             PROPN           NOUN           \n",
            "âŒ Ù…Ú©Ø±Ù…            PROPN           ADJ            \n",
            "âŒ Ø§Ø³Ù„Ø§Ù…           PROPN           NOUN           \n",
            "âŒ Ø¬Ù‡Ø§Ù†            NOUN            PROPN          \n",
            "âŒ Ø¹Ø²Ø§Ø¯Ø§Ø±          ADJ             NOUN           \n",
            "âŒ Ù…Ø§ØªÙ…            NOUN            VERB           \n",
            "âœ“ Ø´               PRON            PRON           \n",
            "âœ“ Ø´Ø¯              VERB            VERB           \n",
            "âœ“ .               PUNCT           PUNCT          \n",
            "\n",
            "[ERROR SENTENCE 2]\n",
            "  WORD            TRUE TAG        PREDICTED TAG  \n",
            "  ---------------------------------------------\n",
            "âœ“ Ø§ÛŒÙ†             DET             DET            \n",
            "âŒ Ù…ÛŒÙ‡Ù…Ø§Ù†ÛŒ         NOUN            ADJ            \n",
            "âœ“ Ø¨Ù‡              ADP             ADP            \n",
            "âœ“ Ù…Ù†Ø¸ÙˆØ±           NOUN            NOUN           \n",
            "âœ“ Ø¢Ø´Ù†Ø§ÛŒÛŒ          NOUN            NOUN           \n",
            "âœ“ Ù‡Ù…â€ŒØªÛŒÙ…ÛŒâ€ŒÙ‡Ø§ÛŒ     NOUN            NOUN           \n",
            "âœ“ Ø§Ùˆ              PRON            PRON           \n",
            "âœ“ Ø¨Ø§              ADP             ADP            \n",
            "âœ“ ØºØ°Ø§Ù‡Ø§ÛŒ          NOUN            NOUN           \n",
            "âœ“ Ø§ÛŒØ±Ø§Ù†ÛŒ          ADJ             ADJ            \n",
            "âœ“ ØªØ±ØªÛŒØ¨           NOUN            NOUN           \n",
            "âœ“ Ø¯Ø§Ø¯Ù‡            VERB            VERB           \n",
            "âœ“ Ø´Ø¯              VERB            VERB           \n",
            "âœ“ .               PUNCT           PUNCT          \n",
            "\n",
            "[ERROR SENTENCE 3]\n",
            "  WORD            TRUE TAG        PREDICTED TAG  \n",
            "  ---------------------------------------------\n",
            "âœ“ Ø¨Ø§              ADP             ADP            \n",
            "âœ“ Ù†Ø²Ø¯ÛŒÚ©           ADJ             ADJ            \n",
            "âœ“ Ø´Ø¯Ù†             NOUN            NOUN           \n",
            "âœ“ Ù…Ù‡Ù„Øª            NOUN            NOUN           \n",
            "âŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ        ADJ             NOUN           \n",
            "âŒ Ø¯ÙˆÙ„Øª            PROPN           NOUN           \n",
            "âŒ Ø±ÙˆØ³ÛŒÙ‡           PROPN           NOUN           \n",
            "âœ“ ØŒ               PUNCT           PUNCT          \n",
            "âœ“ Ù…Ø¨Ù†ÛŒ            ADJ             ADJ            \n",
            "âœ“ Ø¨Ø±              ADP             ADP            \n",
            "âœ“ ØªØ³Ù„ÛŒÙ…           NOUN            NOUN           \n",
            "âœ“ Ù†ÛŒØ±ÙˆÙ‡Ø§ÛŒ         NOUN            NOUN           \n",
            "âŒ Ú†Ú†Ù†ÛŒ            ADJ             NOUN           \n",
            "âœ“ ØŒ               PUNCT           PUNCT          \n",
            "âœ“ Ø¯Ø±Ú¯ÛŒØ±ÛŒâ€ŒÙ‡Ø§ÛŒ      NOUN            NOUN           \n",
            "âŒ Ú¯Ø±ÙˆÙ‡            PROPN           NOUN           \n",
            "âŒ Ø¬Ø¯Ø§ÛŒÛŒâ€ŒØ·Ù„Ø¨       PROPN           NOUN           \n",
            "âœ“ Ø¨Ø§              ADP             ADP            \n",
            "âŒ Ø¯ÙˆÙ„Øª            PROPN           NOUN           \n",
            "âŒ Ø±ÙˆØ³ÛŒÙ‡           PROPN           NOUN           \n",
            "âœ“ Ø¨ÛŒØ´ØªØ±           ADJ             ADJ            \n",
            "âœ“ Ø´Ø¯Ù‡             VERB            VERB           \n",
            "âœ“ Ø§Ø³Øª             AUX             AUX            \n",
            "âœ“ .               PUNCT           PUNCT          \n",
            "\n",
            "[ERROR SENTENCE 4]\n",
            "  WORD            TRUE TAG        PREDICTED TAG  \n",
            "  ---------------------------------------------\n",
            "âœ“ Ø¨Ù‡              ADP             ADP            \n",
            "âœ“ Ù†Ø¸Ø±             NOUN            NOUN           \n",
            "âœ“ Ø§Ùˆ              PRON            PRON           \n",
            "âœ“ Ø·Ø±Ø­             NOUN            NOUN           \n",
            "âœ“ Ùˆ               CCONJ           CCONJ          \n",
            "âœ“ Ú¯Ù†Ø¬Ø§Ù†Ø¯Ù†         NOUN            NOUN           \n",
            "âœ“ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª         NOUN            NOUN           \n",
            "âœ“ ØºÛŒØ±Ù†Ø¸Ø§Ù…ÛŒ        ADJ             ADJ            \n",
            "âœ“ Ø¯Ø±              ADP             ADP            \n",
            "âœ“ Ø¯Ø³ØªÙˆØ±           NOUN            NOUN           \n",
            "âœ“ Ú©Ø§Ø±             NOUN            NOUN           \n",
            "âœ“ Ø§Ù…Ù†ÛŒØªÛŒ          ADJ             ADJ            \n",
            "âœ“ \"               PUNCT           PUNCT          \n",
            "âœ“ Ø§Ù†Ø³Ø¬Ø§Ù…          NOUN            NOUN           \n",
            "âŒ ÙÚ©Ø±ÛŒ            ADJ             NOUN           \n",
            "âœ“ Ùˆ               CCONJ           CCONJ          \n",
            "âŒ Ù†Ø¸Ø±ÛŒ            ADJ             NOUN           \n",
            "âœ“ \"               PUNCT           PUNCT          \n",
            "âœ“ Ø§ÛŒÙ†             DET             DET            \n",
            "âœ“ Ø±Ø´ØªÙ‡Ù”           NOUN            NOUN           \n",
            "âŒ Ù…Ø·Ø§Ù„Ø¹Ø§ØªÛŒ        ADJ             NOUN           \n",
            "âœ“ Ø±Ø§              ADP             ADP            \n",
            "âœ“ Ø§Ø²              ADP             ADP            \n",
            "âŒ Ù‡Ù…              PRON            ADV            \n",
            "âœ“ Ù…ÛŒâ€ŒÙ¾Ø§Ø´Ø¯         VERB            VERB           \n",
            "âœ“ .               PUNCT           PUNCT          \n",
            "\n",
            "[ERROR SENTENCE 5]\n",
            "  WORD            TRUE TAG        PREDICTED TAG  \n",
            "  ---------------------------------------------\n",
            "âŒ ÛŒØ¹Ù†ÛŒ            INTJ            ADJ            \n",
            "âœ“ Ù…Ø§              PRON            PRON           \n",
            "âœ“ Ø¨Ù‡              ADP             ADP            \n",
            "âœ“ Ø§ÛŒÙ†             DET             DET            \n",
            "âœ“ ØªØ±ØªÛŒØ¨           NOUN            NOUN           \n",
            "âœ“ ÙÙ‚Ø·             ADV             ADV            \n",
            "âœ“ ÛŒÚ©              NUM             NUM            \n",
            "âœ“ Ø³Ø·Ù„             NOUN            NOUN           \n",
            "âœ“ Ø¢Ø¨              NOUN            NOUN           \n",
            "âœ“ Ø±Ø§              ADP             ADP            \n",
            "âœ“ Ø¯Ø±              ADP             ADP            \n",
            "âœ“ Ú©ÙˆÛŒØ±            NOUN            NOUN           \n",
            "âœ“ Ù¾Ø§Ø´ÛŒØ¯Ù‡â€ŒØ§ÛŒÙ…      VERB            VERB           \n",
            "âœ“ .               PUNCT           PUNCT          \n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "index 100 is out of bounds for axis 1 with size 100",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2416455065.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;31m# 1. Evaluate on Development Set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_dev_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_dev_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Development (Dev)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;31m# 2. Evaluate on Test Set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2416455065.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, X_data_tf, y_true_padded_idx, tag_tokenizer, original_data_list, dataset_name)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m     \u001b[0mvisualize_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_data_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true_padded_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_padded_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2416455065.py\u001b[0m in \u001b[0;36mvisualize_errors\u001b[0;34m(original_sentences, y_true_padded_idx, y_pred_padded_idx, tag_tokenizer, max_samples)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0mtrue_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_true_padded_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0mpred_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred_padded_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 100 is out of bounds for axis 1 with size 100"
          ]
        }
      ],
      "source": [
        "import pyconll\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gc\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, TimeDistributed, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "# CONFIGURATION AND PATHS\n",
        "TRAIN_PATH = 'fa_perdt-ud-train.conllu'\n",
        "DEV_PATH = 'fa_perdt-ud-dev.conllu'\n",
        "TEST_PATH = 'fa_perdt-ud-test.conllu'\n",
        "\n",
        "# Hyperparameters\n",
        "MAX_LEN = 100         # Sequence length (words per sentence)\n",
        "# reduced this to 20 to reduce ram util\n",
        "FEATURE_VECTOR_DIM = 20\n",
        "#from 64 to 32\n",
        "FEATURE_EMBEDDING_DIM = 32\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Global feature dictionary for indexing features\n",
        "feature_to_index = defaultdict(lambda: len(feature_to_index) + 1) # Start index at 1 for padding\n",
        "# Reserve index 0 for padding\n",
        "\n",
        "# Load\n",
        "def load_conllu(path):\n",
        "    \"\"\"Loads CoNLL-U files and extracts sentences as lists of (words, tags).\"\"\"\n",
        "    data = []\n",
        "    try:\n",
        "        conll = pyconll.load_from_file(path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {path}. Returning empty data.\")\n",
        "        return []\n",
        "\n",
        "    for sentence in conll:\n",
        "        words = [token.form for token in sentence if token.form and token.upos]\n",
        "        tags = [token.upos for token in sentence if token.form and token.upos]\n",
        "\n",
        "        if len(words) == len(tags) and len(words) > 0:\n",
        "            data.append((words, tags))\n",
        "    return data\n",
        "\n",
        "print(\"Loading data...\")\n",
        "train_data = load_conllu(TRAIN_PATH)\n",
        "dev_data = load_conllu(DEV_PATH)\n",
        "test_data = load_conllu(TEST_PATH)\n",
        "\n",
        "X_train_list = [sent[0] for sent in train_data]\n",
        "X_dev_list = [sent[0] for sent in dev_data]\n",
        "X_test_list = [sent[0] for sent in test_data]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_features_for_word(word):\n",
        "    \"\"\"\n",
        "    MEMM model but this is as light as possible because all other models go out of availbale ram\n",
        "    \"\"\"\n",
        "    features = set()\n",
        "    word_lower = word.lower() #some kind of opt\n",
        "\n",
        "    # Core lexical and morphological features\n",
        "    features.add(f\"word={word_lower}\")\n",
        "    features.add(f\"prefix1={word_lower[:1]}\")\n",
        "    features.add(f\"suffix2={word_lower[-2:]}\")\n",
        "\n",
        "    # Orthographic features\n",
        "    if word.istitle():\n",
        "        features.add(\"is_title\")\n",
        "    if word.isdigit():\n",
        "        features.add(\"is_number\")\n",
        "\n",
        "    return features\n",
        "\n",
        "def featurize_dataset(data_list):\n",
        "    \"\"\"Converts sentences into a 2D matrix of feature indices.\"\"\"\n",
        "    global feature_to_index\n",
        "\n",
        "    all_sentences_features = []\n",
        "\n",
        "    # 1. First pass (for training set only): Build feature vocabulary\n",
        "    is_training_set = all(f in feature_to_index for f in extract_features_for_word(data_list[0][0])) == False\n",
        "    if is_training_set:\n",
        "        print(\"Building feature vocabulary...\")\n",
        "        temp_feature_to_index = defaultdict(lambda: len(temp_feature_to_index) + 1)\n",
        "        # Use a flat list of all words for efficient vocabulary building\n",
        "        all_words = [word for sentence in data_list for word in sentence]\n",
        "\n",
        "        for word in all_words:\n",
        "            features = extract_features_for_word(word)\n",
        "            for f in features:\n",
        "                 temp_feature_to_index[f] # Call defaultdict to populate it\n",
        "\n",
        "        # Merge the temporary dictionary into the global one\n",
        "        feature_to_index.update(temp_feature_to_index)\n",
        "        print(f\"Total unique features extracted: {len(feature_to_index)}\")\n",
        "\n",
        "\n",
        "    # 2. Second pass: Convert words into feature index vectors\n",
        "    for sentence in data_list:\n",
        "        sent_feature_indices = []\n",
        "\n",
        "        for word in sentence:\n",
        "            features = extract_features_for_word(word)\n",
        "\n",
        "            # Convert features to indices (0 if not in vocabulary)\n",
        "            indices = [feature_to_index[f] for f in features if f in feature_to_index]\n",
        "\n",
        "            # Pad/truncate to FEATURE_VECTOR_DIM\n",
        "            padded_indices = indices[:FEATURE_VECTOR_DIM]\n",
        "            while len(padded_indices) < FEATURE_VECTOR_DIM:\n",
        "                padded_indices.append(0) # 0 is the padding index\n",
        "\n",
        "            sent_feature_indices.append(padded_indices)\n",
        "\n",
        "        # Pad sentence to MAX_LEN\n",
        "        while len(sent_feature_indices) < MAX_LEN:\n",
        "            sent_feature_indices.append([0] * FEATURE_VECTOR_DIM)\n",
        "\n",
        "        all_sentences_features.append(sent_feature_indices[:MAX_LEN])\n",
        "\n",
        "    return np.array(all_sentences_features)\n",
        "\n",
        "# --- Perform Feature Extraction ---\n",
        "X_train_features_idx = featurize_dataset(X_train_list)\n",
        "X_dev_features_idx = featurize_dataset(X_dev_list)\n",
        "X_test_features_idx = featurize_dataset(X_test_list)\n",
        "\n",
        "\n",
        "# --- STEP 2.9: PREPROCESSING (Tag Tokenization) ---\n",
        "\n",
        "def preprocess_tags(data, max_len=MAX_LEN, tag_tokenizer=None):\n",
        "    y_list = [sent[1] for sent in data]\n",
        "\n",
        "    if tag_tokenizer is None:\n",
        "        tag_tokenizer = Tokenizer(lower=False)\n",
        "        tag_tokenizer.fit_on_texts(y_list)\n",
        "\n",
        "    y_seq = tag_tokenizer.texts_to_sequences(y_list)\n",
        "    y_padded = pad_sequences(y_seq, maxlen=max_len, padding='post', value=0)\n",
        "\n",
        "    TAG_VOCAB_SIZE = len(tag_tokenizer.word_index) + 1\n",
        "    y_categorical = to_categorical(y_padded, num_classes=TAG_VOCAB_SIZE)\n",
        "\n",
        "    return y_categorical, y_padded, tag_tokenizer\n",
        "\n",
        "y_train_cat, y_train_idx, tag_tokenizer = preprocess_tags(train_data)\n",
        "y_dev_cat, y_dev_idx, _ = preprocess_tags(dev_data, tag_tokenizer=tag_tokenizer)\n",
        "y_test_cat, y_test_idx, _ = preprocess_tags(test_data, tag_tokenizer=tag_tokenizer)\n",
        "\n",
        "TAG_VOCAB_SIZE = len(tag_tokenizer.word_index) + 1\n",
        "\n",
        "# --- Convert to TensorFlow Tensors and Clean Up NumPy Arrays ---\n",
        "X_train_tf = tf.convert_to_tensor(X_train_features_idx, dtype=tf.int32)\n",
        "y_train_tf = tf.convert_to_tensor(y_train_cat, dtype=tf.float32)\n",
        "\n",
        "X_dev_tf = tf.convert_to_tensor(X_dev_features_idx, dtype=tf.int32)\n",
        "y_dev_tf = tf.convert_to_tensor(y_dev_cat, dtype=tf.float32)\n",
        "\n",
        "X_test_tf = tf.convert_to_tensor(X_test_features_idx, dtype=tf.int32)\n",
        "\n",
        "# ğŸ—‘ï¸ Explicitly clean up memory\n",
        "del X_train_features_idx, X_dev_features_idx, X_test_features_idx\n",
        "del y_train_cat, y_dev_cat\n",
        "gc.collect()\n",
        "\n",
        "print(\"\\n--- SHAPE CONFIRMATION ---\")\n",
        "print(f\"X_train_tf shape: {X_train_tf.shape} (Note the small feature dimension: {FEATURE_VECTOR_DIM})\")\n",
        "\n",
        "# --- Create TensorFlow Datasets for robust batching ---\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_tf, y_train_tf)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "dev_dataset = tf.data.Dataset.from_tensor_slices((X_dev_tf, y_dev_tf)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "# --- STEP 3: MODEL DEFINITION (MEMM Approximation) ---\n",
        "\n",
        "def create_memm_approximation_model(feature_vocab_size, tag_vocab_size, max_len, feature_dim, embed_dim):\n",
        "\n",
        "    # Input is a 3D tensor of feature indices: (batch, MAX_LEN, FEATURE_VECTOR_DIM)\n",
        "    input_layer = Input(shape=(max_len, feature_dim,), dtype='int32', name='feature_input')\n",
        "\n",
        "    # 1. Feature Embedding Layer: Converts the sparse feature indices into a dense vector\n",
        "    feature_embedding = TimeDistributed(Embedding(\n",
        "        input_dim=feature_vocab_size,\n",
        "        output_dim=embed_dim,       # Reduced output dimension (32)\n",
        "        input_length=feature_dim,\n",
        "        mask_zero=True\n",
        "    ))(input_layer)\n",
        "\n",
        "    # Global Average Pooling combines the feature embeddings for a single word\n",
        "    dense_word_features = TimeDistributed(GlobalAveragePooling1D())(feature_embedding)\n",
        "\n",
        "    # 2. Log-Linear Classifier (Approximation of MEMM)\n",
        "    output_layer = TimeDistributed(Dense(tag_vocab_size, activation='softmax'))(dense_word_features)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "FEATURE_VOCAB_SIZE = len(feature_to_index) + 1\n",
        "model = create_memm_approximation_model(FEATURE_VOCAB_SIZE, TAG_VOCAB_SIZE, MAX_LEN, FEATURE_VECTOR_DIM, FEATURE_EMBEDDING_DIM)\n",
        "\n",
        "# --- STEP 4: TRAINING ---\n",
        "\n",
        "print(f\"\\nTraining Lightweight MEMM Approximation Model for {EPOCHS} epochs...\")\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=dev_dataset,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --- STEP 5: EVALUATION AND VISUALIZATION ---\n",
        "\n",
        "def visualize_errors(original_sentences, y_true_padded_idx, y_pred_padded_idx, tag_tokenizer, max_samples=5):\n",
        "    \"\"\"Finds and prints up to max_samples sentences with incorrect predictions.\"\"\"\n",
        "\n",
        "    idx_to_tag = {v: k for k, v in tag_tokenizer.word_index.items()}\n",
        "    error_count = 0\n",
        "\n",
        "    print(\"\\n\\n--- Sample Incorrect Predictions ---\")\n",
        "\n",
        "    for i in range(len(original_sentences)):\n",
        "        true_tags = []\n",
        "        pred_tags = []\n",
        "        words = original_sentences[i]\n",
        "\n",
        "        for j in range(len(words)):\n",
        "            true_idx = y_true_padded_idx[i, j]\n",
        "            pred_idx = y_pred_padded_idx[i, j]\n",
        "\n",
        "            if true_idx != 0:\n",
        "                true_tags.append(idx_to_tag.get(true_idx, 'UNK'))\n",
        "                pred_tags.append(idx_to_tag.get(pred_idx, 'UNK'))\n",
        "\n",
        "        has_error = any(t != p for t, p in zip(true_tags, pred_tags))\n",
        "\n",
        "        if has_error and error_count < max_samples:\n",
        "            error_count += 1\n",
        "            print(f\"\\n[ERROR SENTENCE {error_count}]\")\n",
        "\n",
        "            print(\"  {:<15} {:<15} {:<15}\".format(\"WORD\", \"TRUE TAG\", \"PREDICTED TAG\"))\n",
        "            print(\"  \" + \"-\"*45)\n",
        "\n",
        "            for word, true_tag, pred_tag in zip(words, true_tags, pred_tags):\n",
        "                prediction_status = \"âŒ\" if true_tag != pred_tag else \"âœ“\"\n",
        "                print(f\"{prediction_status} \" + \"{:<15} {:<15} {:<15}\".format(word, true_tag, pred_tag))\n",
        "\n",
        "def evaluate_model(model, X_data_tf, y_true_padded_idx, tag_tokenizer, original_data_list, dataset_name='set'):\n",
        "    \"\"\"Performs tagging and prints metrics, including error visualization.\"\"\"\n",
        "\n",
        "    y_pred_proba = model.predict(X_data_tf, verbose=0)\n",
        "\n",
        "    y_pred_padded_idx = np.argmax(y_pred_proba, axis=-1)\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for true_seq, pred_seq in zip(y_true_padded_idx, y_pred_padded_idx):\n",
        "        for true_idx, pred_idx in zip(true_seq, pred_seq):\n",
        "            if true_idx != 0:\n",
        "                y_true.append(tag_tokenizer.index_word.get(true_idx, 'UNK'))\n",
        "                y_pred.append(tag_tokenizer.index_word.get(pred_idx, 'UNK'))\n",
        "\n",
        "    print(f'\\n{\"=\"*10} Evaluation on {dataset_name} Set {\"=\"*10}')\n",
        "\n",
        "    if not y_true:\n",
        "        print(\"No valid data for evaluation.\")\n",
        "        return\n",
        "\n",
        "    print('Overall Tagging Accuracy:', accuracy_score(y_true, y_pred))\n",
        "\n",
        "    print('\\nDetailed Metrics (Precision, Recall, F1-Score) per UPOS Tag:')\n",
        "\n",
        "    target_names = sorted(list(set(y_true)))\n",
        "    print(classification_report(y_true, y_pred, labels=target_names, target_names=target_names, digits=4, zero_division=0))\n",
        "    print('='*50)\n",
        "\n",
        "    visualize_errors(original_data_list, y_true_padded_idx, y_pred_padded_idx, tag_tokenizer)\n",
        "\n",
        "\n",
        "# Call the functions\n",
        "\n",
        "# 1. Evaluate on Development Set\n",
        "evaluate_model(model, X_dev_tf, y_dev_idx, tag_tokenizer, X_dev_list, 'Development (Dev)')\n",
        "\n",
        "# 2. Evaluate on Test Set\n",
        "evaluate_model(model, X_test_tf, y_test_idx, tag_tokenizer, X_test_list, 'Final Test')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
